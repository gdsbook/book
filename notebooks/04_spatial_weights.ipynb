{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-appearance",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-citizen",
   "metadata": {},
   "source": [
    "# Spatial Weights\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Spatial weights are widely used in geographic data science to represent geographical relationships between the observational units in a spatially referenced dataset. By expressing the notion of geographical proximity or connectedness, spatial weights are the main mechanism through which the spatial information in geographical data is brought to bear in the subsequent analysis.\n",
    "\n",
    "For example, proximity and adjacency are common spatial questions:\n",
    "\n",
    "    What neighborhoods surround yours?\n",
    "    How many gas stations are within 5 miles of my stalled car? \n",
    "\n",
    "As stated above, these are spatial questions that target specific information about the spatial relationships between a specific target (\"your neighborhood,\" \"my stalled car\") and surrounding relevant sites (\"nearby gas stations,\" \"adjacent neighborhoods\"). However, for us to use this information in statistical analysis, it's often necessary to have computed these relationships between all pairs of observations. This means that, for geographic data science, we are building a *topology*---a mathematical structure that expresses the geometric and spatial relationships between observations---that we can use to examine the data. Spatial weights provide this topology, letting us embed all of our observations in space together, rather than asking and answering single questions about your nearby neighborhoods. \n",
    "\n",
    "Since they provide a way to represent these spatial relationships, spatial weights are widely utilized throughout spatial and geographic data science.\n",
    "In what follows, we first consider different approaches to construct spatial weights, distinguishing between those based on contiguity/adjacency relations from weights obtained from distance based relationships. We then discuss the case of hybrid weights which combine one or more spatial operations in deriving the neighbor relationships between observations. We illustrate all of these concepts through the spatial weights class in PySAL which provides a rich set of methods and characteristics for spatial weights. We also demonstrate its functionality regarding set theory, which permits the derivation of weights through application of notions from set theory. Throughout the chapter, we discuss common file formats used to store spatial weights of different types, and we include visual discussion of spatial weights, making these sometimes abstract constructs more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pysal.lib import weights\n",
    "from pysal.lib import cg as geometry\n",
    "import contextily\n",
    "import geopandas\n",
    "import seaborn\n",
    "import pandas \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-width",
   "metadata": {},
   "source": [
    "## Contiguity Weights\n",
    "\n",
    "A contiguous pair of spatial units are those who share a common border. At first\n",
    "glance this seems straightforward, however, in practice matters turn out to be\n",
    "not so simple. The first complication is that there are different notions of\n",
    "contiguity to consider. First, though, let's use a simple example of a three-by-three grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get points in a grid\n",
    "l = numpy.arange(3)\n",
    "xs, ys = numpy.meshgrid(l, l)\n",
    "# Set up store\n",
    "polys = []\n",
    "# Generate polygons\n",
    "for x, y in zip(xs.flatten(), ys.flatten()):\n",
    "    poly = Polygon([(x, y), (x+1, y), (x+1, y+1), (x, y+1)])\n",
    "    polys.append(poly)\n",
    "# Convert to GeoSeries\n",
    "polys = geopandas.GeoSeries(polys)\n",
    "gdf = geopandas.GeoDataFrame({'geometry': polys, \n",
    "                        'id': ['P-%s'%str(i).zfill(2) for i in range(len(polys))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-accreditation",
   "metadata": {
    "caption": "A three-by-three grid of squares."
   },
   "outputs": [],
   "source": [
    "ax = gdf.plot(facecolor='w', edgecolor='k')\n",
    "[plt.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x-.25 for p in polys],\n",
    "         [p.centroid.y-.25 for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-spain",
   "metadata": {},
   "source": [
    "A common way to express contiguity/adjacency relationships arises from an analogy to the legal moves that different chess pieces can make. *Rook* contiguity requires that the pair of polygons in\n",
    "question share an *edge*. According to this definition, polygon $0$ would be a rook neighbor of $1$ and $3$, while $1$ would be a rook neighbor with $0$, $2$, and $4$. Applying this rule to all 9 polygons we can model our neighbor relations as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a regular 3x3 lattice and draw it here\n",
    "w = weights.contiguity.Rook.from_dataframe(gdf)\n",
    "w.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-singing",
   "metadata": {},
   "source": [
    "Shown visually, we can see this plotted on top of the same grid of labeled polygons, using red dotted lines showing connections between the polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-cooperative",
   "metadata": {
    "caption": "Grid cells connected by a red line are 'neighbors' under a 'Rook' contiguity rule."
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1, subplot_kw=dict(aspect='equal'))\n",
    "w.plot(gdf, edge_kws=dict(color='r', linestyle=':'), ax =ax)\n",
    "gdf.plot(facecolor='w', edgecolor='k', ax=ax)\n",
    "[ax.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x-.25 for p in polys],\n",
    "         [p.centroid.y-.25 for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-homeless",
   "metadata": {},
   "source": [
    "The  `neighbors` attribute of our PySAL $W$ object encodes the neighbor\n",
    "relationships by expressing the *focal* observation on the left (in the `key` of the dictionary), and expressing the *neighbors* to the *focal* in the list on the right (in the `value` of the dictionary). This representation has computational advantages as it exploits\n",
    "the sparse nature of contiguity weights matrices. \n",
    "\n",
    "More specifically, knowing\n",
    "that the neighbors of polygon $0$ are $3$ and $1$ implies that polygons $2, 4,\n",
    "5, 6, 7, 8$ are not Rook neighbors of 0. As such, there is no reason to store\n",
    "the \"non-neighbor\" information and this results in significant reductions in\n",
    "memory requirements. However, it is possible to create the fully dense\n",
    "representation if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(*w.full()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-deviation",
   "metadata": {},
   "source": [
    "As you can see from the matrix above, most entries are zero. In fact out of all of the possible $9^2=81$ linkages that there could be in this matrix, there are only twenty-four:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-magnitude",
   "metadata": {},
   "source": [
    "Thus, we can save a significant amount of memory and lose no information by storing these sparse representations, which only record the non-zero values. \n",
    "\n",
    "More generally, the spatial weights for our 3-by-3 grid can be represented as a matrix that has 9 rows and 9 columns, matching the number of polygons $(n=9)$. An important thing to note is that geography has more than one dimension. When compared to common representations of relationships *in time* used in data science, using information about spatial relationships can be more complex: spatial relationships are bi-directional, while temporal relationships are unidirectional. Further complicating things, the ordering of the observations in the weights matrix is ambiguous. The first row is not first for a specific reason. Here we simply use the alphanumeric ordering of the unit identifiers to match a polygon with a row or column of the matrix, but any arbitrary rule could be followed and the weights matrix would look different. The graph, however, would be isomorphic.\n",
    "\n",
    "Spatial weights matrices may look familiar to those acquainted with social\n",
    "networks and graph theory in which **adjacency** matrices play a central role in\n",
    "expressing connectivity between nodes. Indeed, spatial weights matrices can be\n",
    "understood as a graph adjacency matrix where each observation is a node and\n",
    "the spatial weight assigned between a pair represents the weight of the edge on\n",
    "a graph connecting the arcs. Sometimes, this is called the **dual graph** or **line graph** of the input geographic data. This is advantageous as geographic data science can\n",
    "borrow from the rich graph theory literature. At the same time, spatial\n",
    "data has numerous distinguishing characteristics that necessitate the\n",
    "development of specialized procedures and concepts in the handling of spatial\n",
    "weights. In this chapter we will cover many of these features.\n",
    "\n",
    "A close inspection of our graph reveals that the Rook contiguity criterion actually places\n",
    "a restriction on the spatial relation. More specifically, polygons $0$ and $5$\n",
    "are not Rook neighbors, but they do in fact share a common border. However, in\n",
    "this instance the sharing is due to a common *vertex* rather than a shared\n",
    "*edge*. This leads to the more inclusive notion of *Queen* contiguity that\n",
    "requires the pair of polygons to only share one or more *vertices*. We can create the\n",
    "neighbor relations for this same configuration as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a regular 3x3 lattice and draw it here\n",
    "w = weights.contiguity.Queen.from_dataframe(gdf)\n",
    "w.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-johnston",
   "metadata": {},
   "source": [
    "In addition to this neighbors representation, we can also express the graph visually, as done before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-colonial",
   "metadata": {
    "caption": "Grid cells connected by a red line are considered 'neighbors' under 'Queen' contiguity."
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1, subplot_kw=dict(aspect='equal'))\n",
    "w.plot(gdf, edge_kws=dict(color='r', linestyle=':'), ax =ax)\n",
    "gdf.plot(facecolor='w', edgecolor='k', ax=ax)\n",
    "[ax.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x-.25 for p in polys],\n",
    "         [p.centroid.y-.25 for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-optics",
   "metadata": {},
   "source": [
    "By using `Contiguity.Queen` rather than `Contiguity.Rook`, we consider observations that share a vertex to be neighbors. The result is that the neighbors of $0$ now include $4$ along with $3$ and $1$.\n",
    "\n",
    "Like the `neighbors` dictionary encodes the contiguity relations, the `weights` dictionary encodes the strength of the link connecting the focal to each neighbor. For contiguity\n",
    "weights, values are usually binary and, as in any `PySAL` `W` object, contained in its\n",
    "`weights` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-astrology",
   "metadata": {},
   "source": [
    "Similar to the `neighbors` attribute, the `weights` object is a Python\n",
    "dictionary that only stores the non-zero weights. Although the weights for a\n",
    "given observations neighbors are all the same value for contiguity weights, it\n",
    "is important to note that the `weights` and `neighbors` are aligned with one another; for each observation, its first neighbor in `neighbors` has the first weight in its `weights` entry. This will be important when we examine distance based weights further\n",
    "on, when observations will have different weights. \n",
    "\n",
    "In addition to the `neighbor` and `weights` attributes, the `w` object has a\n",
    "large number of other attributes and methods that can be useful. The\n",
    "`cardinalities` attribute reports the number of neighbors for each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.cardinalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-convention",
   "metadata": {},
   "source": [
    "The related `histogram` attribute provides an overview of the distribution of\n",
    "these cardinalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-argentina",
   "metadata": {},
   "source": [
    "We can obtain a quick visual representation by converting the cardinalities\n",
    "into a `pandas.Series` and creating a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-strain",
   "metadata": {
    "caption": "Histogram of  cardinalities (i.e. the number of neighbors each cell has) in the Queen grid."
   },
   "outputs": [],
   "source": [
    "pandas.Series(w.cardinalities).plot.hist(color='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-harris",
   "metadata": {},
   "source": [
    "The `cardinalities` and `histogram` attributes help quickly spot asymmetries in\n",
    "the number of neighbors. This, as we will see later in the book, is relevant\n",
    "when using spatial weights in other analytical techniques (e.g.\n",
    "spatial autocorrelation analysis or spatial regression). Here we see that there are four corner\n",
    "observations with three neighbors, four edge observations with five neighbors,\n",
    "and the one central observation has eight neighbors. There are also no\n",
    "observations with four, six, or seven neighbors.\n",
    "\n",
    "By convention, an ordered pair of contiguous observations constitutes a *join*\n",
    "represented by a non-zero weight in a $W$. The attribute `s0` records the number\n",
    "of joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-muscle",
   "metadata": {},
   "source": [
    "The `pct_nonzero` attribute provides a measure of the density (compliment of\n",
    "sparsity) of the spatial weights matrix (if we had it stored explicitly, which\n",
    "we don't):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-duplicate",
   "metadata": {},
   "source": [
    "which is equal to $100 \\times (\\texttt{w.s0} / \\texttt{w.n}^2)$.\n",
    "\n",
    "### Spatial Weights from real-world data\n",
    "\n",
    "The regular lattice map encountered above helps us to understand the logic and\n",
    "properties of PySAL's spatial weights class. However, the artificial nature of\n",
    "that geography is of limited relevance to real world research problems.\n",
    "Fortunately, PySAL supports the construction of spatial weights objects from a\n",
    "number of commonly used spatial data formats. Here we demonstrate this\n",
    "functionality for the case of census tracts in San Diego, California. Most spatial\n",
    "data formats, such as shapefiles, are non-topological in that they encode the\n",
    "polygons as a collection of vertices defining the edges of the geometry's\n",
    "boundary. No information about the neighbor relations is explicitly encoded, so we\n",
    "must construct it ourselves. Under the hood, PySAL uses efficient spatial indexing\n",
    "structures to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "san_diego_tracts = geopandas.read_file('../data/sandiego/sandiego_tracts.gpkg')\n",
    "wq = weights.contiguity.Queen.from_dataframe(san_diego_tracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-bahrain",
   "metadata": {},
   "source": [
    "Like before, we can visualize the adjacency relationships, but they are much more difficult to see without showing a closer detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-clinton",
   "metadata": {
    "caption": "The Queen contiguity graph for San Diego tracts. Tracts connected with a red line are neighbors."
   },
   "outputs": [],
   "source": [
    "ax = san_diego_tracts.plot(edgecolor='k', facecolor='w')\n",
    "wq.plot(san_diego_tracts, ax=ax, \n",
    "        edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "        node_kws=dict(marker=''))\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-intent",
   "metadata": {},
   "source": [
    "So, showing more detail, we can get a glimpse of the complicated structure of the contiguity relationships between tracts in the center city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-palmer",
   "metadata": {
    "caption": "An inset view of the Queen contiguity graph."
   },
   "outputs": [],
   "source": [
    "ax = san_diego_tracts.plot(edgecolor='k', facecolor='w')\n",
    "f,ax = wq.plot(san_diego_tracts, ax=ax, \n",
    "        edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "        node_kws=dict(marker=''))\n",
    "ax.axis([-13040000,  -13020000, 3850000, 3860000])\n",
    "#ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-uganda",
   "metadata": {},
   "source": [
    "The weights object for San Diego tracts have the same attributes and methods as\n",
    "we encountered with our artificial layout above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wq.n)\n",
    "print(wq.pct_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-washington",
   "metadata": {},
   "source": [
    "First we have a larger number of spatial units. The spatial weights are\n",
    "also much more sparse for the tracts than what we saw for our smaller toy\n",
    "layout. Moreover, the cardinalities have a radically different distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-harvest",
   "metadata": {
    "caption": "Cardinalities for the Queen contiguity graph among San Diego Tracts"
   },
   "outputs": [],
   "source": [
    "s = pandas.Series(wq.cardinalities)\n",
    "s.plot.hist(bins=s.unique().shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-rainbow",
   "metadata": {},
   "source": [
    "As the minimum number of neighbors is 1, while there is one polygon with 29\n",
    "queen neighbors. The most common number of neighbors is 6.\n",
    "\n",
    "There is also a function to create the rook weights for the same dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-cargo",
   "metadata": {
    "caption": "Cardinalities for the Rook contiguity graph among San Diego Tracts"
   },
   "outputs": [],
   "source": [
    "wr = weights.contiguity.Rook.from_dataframe(san_diego_tracts)\n",
    "print(wr.pct_nonzero)\n",
    "s = pandas.Series(wr.cardinalities)\n",
    "s.plot.hist(bins=s.unique().shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-brother",
   "metadata": {},
   "source": [
    "The cardinality histogram shifts downward due to the increasing sparsity of the\n",
    "weights for the rook case relative to the queen criterion. Conceptually, this makes sense; all Rook neighbors are also Queen neighbors, since Queen includes neighbors that share an edge. But, not all Queen neighbors are Rook neighbors, since some Queen neighbors only share a point on their boundaries in common. \n",
    "\n",
    "The example above shows how the notion of contiguity, although more\n",
    "straightforward in the case of a grid, can be naturally extended beyond the\n",
    "particular case of a regular lattice. The principle to keep in mind is that we\n",
    "consider contiguous (and hence call neighbors) observations which share part\n",
    "of their border coordinates. In the queen case, a single point is enough to make\n",
    "the join. For rook neighbors, we require a join to consist of one or more\n",
    "shared edges. This distinction is probably less relevant in the real world than\n",
    "it appears in the grid example above, and it is probably down to geocoding\n",
    "issues rather than substantive differences. In any case, there are special cases\n",
    "where this distinction can matter and it is useful to be familiar with the\n",
    "differences between the two approaches and how to apply them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-refund",
   "metadata": {},
   "source": [
    "## Distance Based Weights\n",
    "\n",
    "In addition to contiguity, we can also define neighbor relations as a function of\n",
    "the distance separating spatial observations. Usually, this means that a matrix expressing the distances between all pairs of observations are required. These are then provided to a **kernel** function which uses the proximity information to model proximity as a smooth function of distance. PySAL implements a family of\n",
    "distance functions. Here we illustrate a selection beginning with the notion\n",
    "of *nearest neighbor* weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-melbourne",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor weights\n",
    "\n",
    "The first type of distance based weights defines the neighbor set of a\n",
    "particular observation as containing its nearest $k$ observations, where the\n",
    "user specifies the value of $k$. To illustrate this for the San Diego\n",
    "tracts we take $k=4$. This still leaves the issue of how to measure the distance\n",
    "between these polygon objects, however. To do so we develop a representative\n",
    "point for each of the polygons using the so called \"center of mass\" or centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4 = weights.distance.KNN.from_dataframe(san_diego_tracts, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-socket",
   "metadata": {},
   "source": [
    "The centroids are attributes of the polygon shapes that PySAL calculates from\n",
    "the spatial information stored in the `GeoDataFrame`. Since we are dealing with\n",
    "polygons in this case, PySAL uses inter-centroid distances to determine the\n",
    "$k$ nearest observations to each polygon. \n",
    "\n",
    "The k-nearest neighbor weights displays no island problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-geometry",
   "metadata": {},
   "source": [
    "This is the same for the contiguity case above but, in the case of k-nearest neighbor weights, this is by construction. However, examination of the cardinality histogram for the k-nearest neighbor weights shows us\n",
    "that each observation has the same number of neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-sapphire",
   "metadata": {},
   "source": [
    "This is also by construction as the feature is at the very heart of the\n",
    "k-nearest neighbor approach. In some cases, this is not an issue but a desired feature. In\n",
    "other contexts, however, this characteristic of k-nearest neighbor weights can be undesirable.\n",
    "In such situations, we can turn to other types of distance-based weights.\n",
    "\n",
    "### Kernel weights\n",
    "\n",
    "The k-nearest neighbor rule assigns binary values to the weights for neighboring observations.\n",
    "PySAL also supports continuously valued weights to reflect Tobler's first law\n",
    "{cite}`Tobler1970computer` in a more direct way: observations that are close to a unit have larger\n",
    "valued weights than more distant observations.\n",
    "\n",
    "Kernel weights are one of the most commonly-used kinds of distance weights. They\n",
    "reflect the case where similarity/spatial proximity is assumed or expected to\n",
    "decay with distance. The essence of kernel weights is that the weight between\n",
    "observations $i$ and $j$ is based on their distance, but this is modulated by\n",
    "a kernel function with certain properties. PySAL implements several kernels.\n",
    "All of them share the properties of distance decay (thus encoding Tobler's First \n",
    "Law), but may decay at different rates with respect to distance.\n",
    "\n",
    "As a computational note, it is worth mentioning that many of these distance-based decay functions require more computation than the contiguity weights or K-nearest neighbor weights discussed above. This is because the contiguity & k-nearest neighbor structures embed simple assumptions about how shapes relate in space, while kernel functions relax several of those assumptions. Thus, they provide more flexibility at the expense of computation.\n",
    "\n",
    "The simplest way to compute Kernel weights in PySAL involves a single function\n",
    "call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel = weights.distance.Kernel.from_dataframe(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-contractor",
   "metadata": {},
   "source": [
    "Like k-nearest neighbor weights, the Kernel weights are based on distances between observations. By default, if the input data is an areal unit, we use a central representative point (like the centroid) for that polygon.\n",
    "The value of the weights will be a function of two main options for\n",
    "kernel weights: choice of kernel function; and the bandwidth. The\n",
    "former controls how distance between $i$ and $j$ is \"modulated\" to produce a\n",
    "the weight that goes in $w_{ij}$. In this respect, PySAL offers a large number\n",
    "of functions that determine the shape of the distance\n",
    "decay function. The bandwidth specifies the distance from each focal unit over which\n",
    "the kernel function is applied. For observations separated by distances larger\n",
    "than the bandwidth, the weights are set to zero.\n",
    "\n",
    "The default values for kernels are to use a triangular kernel with a bandwidth distance\n",
    "equal to the maximum knn=2 distance\n",
    "for all observations. The latter implies a so-called fixed bandwidth where all\n",
    "observations use the same distance for the cut-off. We can inspect this from\n",
    "the generated `W` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-emission",
   "metadata": {},
   "source": [
    "for the kernel function, and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.bandwidth[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-craps",
   "metadata": {},
   "source": [
    "For the bandwidth applied to each observation.\n",
    "\n",
    "Although simple, a fixed bandwidth is not always the best choice. For example,\n",
    "in cases where the density of the observations varies over the study region,\n",
    "using the same threshold anywhere will result in regions with a high density\n",
    "of neighbors while others with observations very sparsely connected. In these\n",
    "situations, an *adaptive* bandwidth -one which varies by observation and its\n",
    "characteristics- can be preferred. Adaptive bandwidths are picked again using a K-nearest neighbor rule. A bandwidth for each observation is chosen such that, once the $k$-nearest observation is considered, all the remaining observations have zero weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-power",
   "metadata": {},
   "source": [
    "For example, using a subset of tracts in our San Diego dataset, we can see that the centroids of each tract are not exactly regularly-spaced, although others do nearly fall into a regular spacing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-pledge",
   "metadata": {
    "caption": "Centroids of some tracts in San Diego are (nearly) evenly spaced."
   },
   "outputs": [],
   "source": [
    "sub_30 = san_diego_tracts.query(\"sub_30 == True\")\n",
    "ax = sub_30.plot(facecolor='w', edgecolor='k')\n",
    "sub_30.head(30).centroid.plot(color='r', ax=ax)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-seventh",
   "metadata": {},
   "source": [
    "We can see that the adaptive bandwidth adjusts for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_adaptive = weights.distance.Kernel.from_dataframe(sub_30, fixed=False, k=15)\n",
    "w_adaptive.bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-violence",
   "metadata": {},
   "source": [
    "And, we can visualize what these kernels look like on the map, too, by focusing on an individual unit and showing how the distance decay attenuates the weight by grabbing the corresponding row of the full kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-fitness",
   "metadata": {
    "caption": "A Gaussian kernel centered on two different tracts."
   },
   "outputs": [],
   "source": [
    "full_matrix, ids = w_adaptive.full() \n",
    "f,ax = plt.subplots(1,2,figsize=(12,6), subplot_kw=dict(aspect='equal'))\n",
    "sub_30.assign(weight_0 = full_matrix[0]).plot(\"weight_0\", cmap='plasma', ax=ax[0])\n",
    "sub_30.assign(weight_15 = full_matrix[17]).plot(\"weight_15\", cmap='plasma', ax=ax[1])\n",
    "ax[0].set_title(\"Kernel centered on first tract\")\n",
    "ax[1].set_title(\"Kernel centered on 18th tract\")\n",
    "[ax_.set_axis_off() for ax_ in ax]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-extension",
   "metadata": {},
   "source": [
    "What the kernel looks like can be strongly affected by the structure of spatial proximity, so any part of the map can look quite different from any other part of the map. By imposing a clear distance decay over several of the neighbors of each observation,\n",
    "kernel weights incorporate Tobler's law very explicitly. Often, this comes at the cost of\n",
    "increased memory requirements, as every single pair of observations within the\n",
    "bandwidth distance is considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-nothing",
   "metadata": {},
   "source": [
    "In many instances, this may be at odds with the nature of the spatial\n",
    "interactions at hand, which operate over a more limited range of distance. In\n",
    "these cases, expanding the neighborhood set beyond might lead us to consider\n",
    "interactions which either do not take place, or are inconsequential. Thus, for\n",
    "both substantive and computational reasons, it might make sense to further\n",
    "limit the range, keeping impacts to be hyper-local.\n",
    "\n",
    "## Distance bands and hybrid Weights\n",
    "\n",
    "In some contexts, it makes sense to draw a circle around each observation and\n",
    "consider as neighbors every other observation that falls within the circle.\n",
    "In the GIS terminology, this is akin to drawing a buffer around each point and\n",
    "performing a point-in-polygon operation that determines whether each of the\n",
    "other observations are within the buffer. If they are, they are assigned a\n",
    "weight of one in the spatial weights matrix, if not they receive a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bdb = weights.distance.DistanceBand.from_dataframe(gdf, 1.5, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-toronto",
   "metadata": {},
   "source": [
    "This creates a binary distance weights where every other observation within\n",
    "a distance of 1.5 is considered neighbor.\n",
    "\n",
    "Hybrid weights, also available in PySAL, are matrices that\n",
    "offer a blend of the threshold and continuous distance weights by truncating the\n",
    "neighbor pairs to those separated by less than some distance threshold.\n",
    "\n",
    "Let us for instance calculate a hybrid matrix that combines inverse distance\n",
    "weights up to a certain threshold and then truncate the weights to zero for\n",
    "everyone else. For this example we will return to the small lattice example\n",
    "covered in the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hy = weights.distance.DistanceBand.from_dataframe(gdf, 1.5, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-blood",
   "metadata": {},
   "source": [
    "We apply a threshold of 1.5 for this illustration. PySAL truncates continuous\n",
    "weights at this distance. It is important to keep in mind that the threshold\n",
    "distance must use the same units of distance as the units used to define the\n",
    "matrix.\n",
    "\n",
    "We can inspect the `weights` dictionary to see the weights, in this case, are\n",
    "not 1 or 0 only, but there are values in-between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hy.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-protein",
   "metadata": {},
   "source": [
    "which contrasts with the original weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-frederick",
   "metadata": {},
   "source": [
    "### Great Circle distances\n",
    "\n",
    "We must make one final curve before leaving the distance based weights. It is important that the\n",
    "calculation of distances between objects takes the curvature of the Earth's\n",
    "surface into account. This can be done before computing the spatial weights object, \n",
    "by transforming the coordinates of data points into a projected reference system, in \n",
    "a GIS, for example. If \n",
    "this is not possible or convenient, an approximation that considers the\n",
    "curvature implicit in non-projected reference systems (e.g.\n",
    "longitude/latitude) can be a sufficient workaround. PySAL provides such\n",
    "approximation as part of its functionality.\n",
    "\n",
    "To illustrate the relevance of ignoring this aspect altogether we will examine\n",
    "distance based weights for the case of counties in the state of Texas. First, let us compute\n",
    "a KNN-4 object that ignores the curvature of the Earth's surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "texas = geopandas.read_file('../data/texas/texas.shp')\n",
    "knn4_bad = weights.distance.KNN.from_dataframe(texas, k=4) # ignore curvature of the earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4_bad.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-video",
   "metadata": {},
   "source": [
    "Next, let us take curvature into account. To do this, we require the\n",
    "radius of the Earth expressed in a given metric. PySAL provides this number\n",
    "in both miles and kilometers. For the sake of the example, we will use miles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = geometry.sphere.RADIUS_EARTH_MILES\n",
    "radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-morning",
   "metadata": {},
   "source": [
    "With this measure at hand, we can pass it to the weights constructor (either\n",
    "straight from a shapefile or from a `GeoDataFrame`) and distances will be\n",
    "expressed in the units we have used for the radius, that is in miles in our\n",
    "case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4 = weights.distance.KNN.from_dataframe(texas, k=4, radius=radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-feelings",
   "metadata": {},
   "source": [
    "Comparing the resulting neighbor sets, we see that ignoring the curvature of the\n",
    "Earth's surface can create erroneous neighbor pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4_bad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-tongue",
   "metadata": {},
   "source": [
    "## Set Operations on Weights\n",
    "\n",
    "So far, we have seen different principles that guide how to build spatial\n",
    "weights matrices. In this section, we explore how we can create new matrices\n",
    "by *combining* different existing ones. This is useful in contexts where a\n",
    "single neighborhood rule has flaws or when theory or other guiding principles\n",
    "point in directions that require combining more than a single criterion.\n",
    "\n",
    "We will explore these ideas in the section by returning to the San Diego tracts.\n",
    "A number of ways exist to expand the basic criteria we have reviewed above and create\n",
    "hybrid or bespoke weights. In this example, we will generate a combination of the original contiguity\n",
    "weights and the nearest neighbor weights. We will examine two different\n",
    "approaches that provide the same solution, thus illustrating the value of set\n",
    "operations in PySAL.\n",
    "\n",
    "### Editing/connecting disconnected observations\n",
    "\n",
    "Imagine one of our tracts was an island and did not have any neighbors in the contiguity case. This can\n",
    "create issues in the spatial analytics that build on spatial weights, so it is good practice\n",
    "to amend the matrix before using it.\n",
    "The first approach we adopt is to find the nearest neighbor for the island observation\n",
    "and then add this pair of neighbors to extend the neighbor pairs from the\n",
    "original contiguity weight to obtain a fully connected set of weights. \n",
    "\n",
    "We will assume, for the sake of the example, that the disconnected observation was number 103. For us to reattach this tract, we can assign it to be \"connected\" to its nearest neighbor. Let's first extract our \"problem\" geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "disconnected_tract = san_diego_tracts.iloc[[103]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-richmond",
   "metadata": {},
   "source": [
    "As we have seen above, this tract *does* have neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq[103]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-correlation",
   "metadata": {},
   "source": [
    "But, for this example, we will assume it does not and thus we find ourselves in the position of having to create additional neighboring units. This approach does not only apply in the context of islands. Sometimes, the process we are interested in may require that we manually edit the weights to better reflect connections we *know* exist.\n",
    "\n",
    "We will connect the observation to its nearest neighbor. To do this, we can construct the KNN graph as we did above, but set `k=1`, so observations are only assigned to their nearest neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk1 = weights.distance.KNN.from_dataframe(san_diego_tracts, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-russell",
   "metadata": {},
   "source": [
    "In this graph, all our observations are connected to one other observation by construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(wk1.cardinalities).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-roommate",
   "metadata": {},
   "source": [
    "So is, of course, our tract of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk1.neighbors[103]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-kitchen",
   "metadata": {},
   "source": [
    "To connect it in our initial matrix, we need to create a copy of the `neighbors` dictionary and update the entry for `103`, including `102` as a neighbor. So, first we copy the neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = wr.neighbors.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-poland",
   "metadata": {},
   "source": [
    "and then we change the entry for the island observation to include its\n",
    "nearest neighbor (`102`) as well as update `102` to have `103` as a neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors[103].append(102)\n",
    "neighbors[102].append(103)\n",
    "w_new = weights.W(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-frequency",
   "metadata": {},
   "source": [
    "### Using the `union` of matrices\n",
    "\n",
    "A more elegant approach to the island problem makes use of PySAL's support for\n",
    "*set theoretic operations* on PySAL weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fixed_sets = weights.set_operations.w_union(wr, wk1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-europe",
   "metadata": {},
   "source": [
    "It is important to mention that this approach is not exactly the same, at least\n",
    "in principle, as the one above. It could be that the nearest\n",
    "observation was not originally a neighbor and, in this case, the resulting\n",
    "matrices would differ. This is a rare but theoretically possible situation.\n",
    "\n",
    "### Block Weights\n",
    "\n",
    "A final type of spatial weight we examine here are block weights. Membership in\n",
    "a group or set defines the neighbor relationships. Block weights connect every\n",
    "observation in a data set that belongs to the same category in a list provided\n",
    "*ex-ante*. Usually, this list will have some relation to geography \n",
    "of the observations but, technically speaking, all one needs to create block\n",
    "weights is a list of memberships. In essence, a hierarchical structure groups\n",
    "individual observations and assigns a value of one to the weight for all\n",
    "pair-members of the group, and a value of zero to pairs involving observations\n",
    "belonging to different groups.\n",
    "\n",
    "\n",
    "To demonstrate this class of spatial weights, we will use the tract dataset for\n",
    "San Diego and focus on their county membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "san_diego_tracts[['GEOID', 'state', 'county', 'tract']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-stocks",
   "metadata": {},
   "source": [
    "Every tract has a unique ID (`GEOID`) and a county ID, shared by all tracts in\n",
    "the same county. Since the entire region of San Diego is in California, the\n",
    "state ID is the same across the dataset.\n",
    "\n",
    "To build a block weights matrix, we do not even need spatial data beyond the\n",
    "list of memberships. In this case, we will use the county membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since this is a large dataset, it might take a while to process\n",
    "w_bl = weights.util.block_weights(san_diego_tracts['county'].values, \n",
    "                                  ids=san_diego_tracts['GEOID'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-arrangement",
   "metadata": {},
   "source": [
    "As a check, let's consider the first two rows in the table above. If the block\n",
    "weights command has worked out correctly, both should be neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "'06073000201' in w_bl['06073000100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-philosophy",
   "metadata": {},
   "source": [
    "We can use block weights as an intermediate step in more involved analyses\n",
    "of spatial linkages. Suppose for example, the researcher wanted to allow for\n",
    "queen neighbors within counties but not for tracts across different counties.\n",
    "Tracts from different counties are not considered neighbors. To create such\n",
    "as spatial weights matrix would require a combination of the queen and the block\n",
    "criteria, and PySAL can implement that blending through one of the set operations.\n",
    "\n",
    "\n",
    "### Visualizing weight set operations\n",
    "\n",
    "We finish the chapter by illustrating the concepts above using \n",
    "the 32 states of Mexico. We compare the neighbor graphs that results\n",
    "from the different criteria used to define neighbor relations. The graphs are\n",
    "constructed by specifying the nodes at the polygon centroids for each of the\n",
    "states. Edges are then drawn between a pair of nodes to reflect a neighbor\n",
    "relation between the states according to the specific definition.\n",
    "\n",
    "We first read in the data for Mexico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-drilling",
   "metadata": {
    "caption": "States in Mexico"
   },
   "outputs": [],
   "source": [
    "mx = geopandas.read_file('../data/mexico/mexicojoin.shp')\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "mx.plot(ax=ax)\n",
    "ax.set_axis_off()\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-calendar",
   "metadata": {},
   "source": [
    "We will contrast the connectivity structure for the three following types of spatial weights:\n",
    "\n",
    "- Queen contiguity weights\n",
    "- Block weights\n",
    "- Combination of Block+Queen weights\n",
    "\n",
    "\n",
    "Beginning with Queen weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-aspect",
   "metadata": {
    "caption": "Queen weights among states in Mexico."
   },
   "outputs": [],
   "source": [
    "queen_mx = weights.contiguity.Queen.from_dataframe(mx)\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "mx.plot(ax=ax)\n",
    "queen_mx.plot(mx,edge_kws=dict(linewidth=1.5, color='orangered'), node_kws=dict(marker='*'),  ax=ax, )\n",
    "ax.set_axis_off()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-withdrawal",
   "metadata": {},
   "source": [
    "For the block weights, we use the official designation of regions from the federal government:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-motivation",
   "metadata": {
    "caption": "Regions of Mexico"
   },
   "outputs": [],
   "source": [
    "ax = mx.plot(column='INEGI2', categorical=True, cmap='Pastel2')\n",
    "block_mx = weights.util.block_weights(mx['INEGI2'].values)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-triple",
   "metadata": {
    "caption": "'Block' weights for Mexican states within regions."
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "mx.plot(column='INEGI2', categorical=True, \n",
    "        cmap='Pastel2', ax=ax)\n",
    "block_mx.plot(mx, edge_kws=dict(linewidth=1.5, \n",
    "                                color='orangered'), \n",
    "                  node_kws=dict(marker='*'), ax=ax)\n",
    "ax.set_axis_off()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-suspect",
   "metadata": {},
   "source": [
    "Next, we construct the union of queen contiguity and block weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-manufacturer",
   "metadata": {
    "caption": "The union of Region-block and Queen weights for Mexican states. In this graph, a state's neighbors are either within the same region *or* are in a different region but sharing a point on the boundary."
   },
   "outputs": [],
   "source": [
    "union_mx = weights.set_operations.w_union(block_mx, queen_mx)\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "mx.plot(column='INEGI2', categorical=True, cmap='Pastel2', ax=ax)\n",
    "union_mx.plot(mx, edge_kws=dict(linewidth=1.5, \n",
    "                                color='orangered'), \n",
    "              node_kws=dict(marker='*'), ax=ax)\n",
    "ax.set_axis_off()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-impact",
   "metadata": {},
   "source": [
    "Finally, we compare the three neighbor graphs side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-portfolio",
   "metadata": {
    "caption": "The three graphs discussed above, shown side-by-side."
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "\n",
    "# Contiguity\n",
    "ax = axs[0]\n",
    "mx.plot(column='INEGI2', categorical=True, \n",
    "        cmap='Pastel2', ax=ax)\n",
    "queen_mx.plot(mx, edge_kws=dict(linewidth=1.5, color='orangered'), \n",
    "              node_kws=dict(marker='*'), ax=ax)\n",
    "ax.set_axis_off()\n",
    "ax.set_xlabel('Queen')\n",
    "ax.axis('equal')\n",
    "\n",
    "# Block\n",
    "ax = axs[1]\n",
    "mx.plot(column='INEGI2', categorical=True, \n",
    "        cmap='Pastel2', ax=ax)\n",
    "block_mx.plot(mx, edge_kws=dict(linewidth=1.5, color='orangered'), \n",
    "              node_kws=dict(marker='*'), ax=ax)\n",
    "ax.set_axis_off()\n",
    "ax.set_xlabel('Block weights')\n",
    "ax.axis('equal')\n",
    "\n",
    "# Union\n",
    "ax = axs[2]\n",
    "mx.plot(column='INEGI2', categorical=True, \n",
    "        cmap='Pastel2', ax=ax)\n",
    "union_mx.plot(mx, edge_kws=dict(linewidth=1.5, color='orangered'), \n",
    "              node_kws=dict(marker='*'), ax=ax)\n",
    "ax.set_axis_off()\n",
    "ax.set_xlabel('Queen + Block')\n",
    "plt.axis('equal')\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-processing",
   "metadata": {},
   "source": [
    "Focusing on the Queen and Block graphs, there are clear distinctions between the\n",
    "connectivity structures. The Block graph is visually more dense in particular areas relative to the\n",
    "Queen graph and this is borne out in their sparsity measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_mx.pct_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_mx.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-quantum",
   "metadata": {},
   "source": [
    "The other distinguishing characteristic can be seen in the number of connected\n",
    "components in the different graphs. The Queen graph has a single connected\n",
    "component - which in graph theory terms, means for all pairs of states there is\n",
    "at least one path of edges that connects the two states. The Block graph has\n",
    "five connected components, one for each of the five regions. Moreover, each of\n",
    "these connected components is fully-connected, meaning there is an edge that\n",
    "directly connects each pair of member states. However, there are no edges\n",
    "between states belonging to different blocks (or components).\n",
    "\n",
    "\n",
    "As we will see in later chapters, certain spatial analytical techniques require\n",
    "a fully connected weights graph. In these cases, we could adopt the Queen\n",
    "definition since this satisfies the single connected component requirement.\n",
    "However, we may wish to use the Union weights graph as that provides a single\n",
    "connected component, but offers a blend of different types of connectivity\n",
    "intensities, with the intra-regional (block) linkages being very dense, while\n",
    "the inter-regional linkages are thinner but provide for the single connected\n",
    "component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-belief",
   "metadata": {},
   "source": [
    "## Use case: Boundary detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-analysis",
   "metadata": {},
   "source": [
    "Spatial weights are ubiquitous in the analysis of spatial patterns in data, since they provide a direct method to represent spatial structure. \n",
    "However, spatial weights are sometimes useful in their own right, such as when examining latent structures directly in the graphs themselves or when using them to conduct descriptive analysis. \n",
    "One clear use case that arises in the analysis of social data is to characterize latent *data discontinuities*. By *data discontinuity*, we mean a single border (or collection of borders) where data for a variate (or many variates) of interest change rapidly. \n",
    "These can be used in stochastic models of boundaries {cite}`Lu2005bayesian,Fitzpatrick2010ecological,Dean2019frontiers` or used to adapt classic empirical outlier detection methods. \n",
    "Below, we'll show one model-free way to identify empirical boundaries in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-testing",
   "metadata": {},
   "source": [
    "First, let's consider the median household income for our census tracts in San Diego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-willow",
   "metadata": {
    "caption": "Household incomes in San Diego."
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2, figsize=(12,4))\n",
    "san_diego_tracts.plot('median_hh_income', ax=ax[0])\n",
    "ax[0].set_aspect('equal')\n",
    "ax[0].set_axis_off()\n",
    "san_diego_tracts['median_hh_income'].hist(ax=ax[1])\n",
    "ax[1].set_title(\"Median Household Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-responsibility",
   "metadata": {},
   "source": [
    "Now, we see some cases where there are very stark differences between neighboring areas, and some cases where there are essentially no difference between adjacent areas. Digging into this, we can examine the *distribution of differences* in neighboring areas using the adjacency list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist = wr.to_adjlist() \n",
    "adjlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-advocacy",
   "metadata": {},
   "source": [
    "This provides us with a table with three columns. `Focal` is the column containing the \"origin\" of the link, `neighbor` is the column containing the \"destination\" of the link, and `weight` contains how strong the link from `focal` to `neighbor` is. Since our weights are *symmetrical*, this table contains two entries per pair of neighbors, one for `(focal,neighbor)` and the other for `(neighbor,focal)`. Using this table and `pandas`, we can merge up the focal units' & neighboring units' median household incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_wealth = adjlist.merge(san_diego_tracts[['median_hh_income']], how='left', \n",
    "                               left_on='focal', right_index=True)\\\n",
    "                        .merge(san_diego_tracts[['median_hh_income']], how='left',\n",
    "                               left_on='neighbor', right_index=True, \n",
    "                               suffixes=('_focal', '_neighbor'))\n",
    "adjlist_wealth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-breeding",
   "metadata": {},
   "source": [
    "Now, we have the wealth at both the focal observation and the neighbor observation. The difference between these two columns provides us every pairwise difference between *adjacent* tracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_wealth['diff'] = adjlist_wealth['median_hh_income_focal'] - adjlist_wealth['median_hh_income_neighbor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-latitude",
   "metadata": {},
   "source": [
    "With this difference information we can do a few things. First, we can compare whether or not this *distribution* is distinct from the distribution of non-neighboring tracts' differences in wealth. \n",
    "\n",
    "To do this, we can first compute the all-pairs differences in wealth using the `numpy.subtract` function. Some functions in dumpy have special functionality; these `ufuncs` (short for \"universal functions\") often support special applications to your data. Here, we will use `numpy.subtract.outer` to take the difference over the \"outer cartesian product\" of two vectors; in practice, this results in the subtraction of all of the combinations of the input vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = numpy.subtract.outer(san_diego_tracts['median_hh_income'].values, san_diego_tracts['median_hh_income'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-monkey",
   "metadata": {},
   "source": [
    "Then, we need to filter out those cells of `all_pairs` that are neighbors. Fortunately, our weights matrix is *binary*. So, subtracting it from an $N \\times N$ matrix of $1$s will result in the *complement* of our original weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "complement_wr = 1 - wr.sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-treasure",
   "metadata": {},
   "source": [
    "Using this complement, we can filter the `all_pairs` matrix to only consider the differences in median household income for tracts that are not neighboring: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_neighboring_diffs = (complement_wr * all_pairs).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-stone",
   "metadata": {},
   "source": [
    "Now, we can compare the two distributions of the difference in wealth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-strap",
   "metadata": {
    "caption": "Diferences between median incomes among neighboring (and non-neighboring) tracts in San Diego."
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,3))\n",
    "plt.hist(non_neighboring_diffs, color='lightgrey', \n",
    "         edgecolor='k', density=True, bins=50, label='Nonneighbors')\n",
    "plt.hist(adjlist_wealth['diff'], \n",
    "         color='salmon', edgecolor='orangered', \n",
    "         linewidth=3, density=True, \n",
    "         histtype='step', bins=50, label='Neighbors')\n",
    "seaborn.despine()\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Dollar Differences ($)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-whole",
   "metadata": {},
   "source": [
    "From this, we can see that the two distributions are distinct, with the distribution of difference in *non-neighboring* tracts being slightly more dispersed than that for *neighboring* tracts. Thus, on the whole, this means that neighboring tracts have more *smaller differences in wealth* than non-neighboring tracts. This is consistent with the behavior we will talk about in later chapters concerning *spatial autocorrelation*, the tendency for observations to be statistically more similar to nearby observations than they are to distant observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-gather",
   "metadata": {},
   "source": [
    "With this, we can then find our *most extreme* observed differences in wealth. Since our links are symmetric, the table sort is symmetric. So, we can focus only on focal observations with *the most extreme* difference in wealth from their immediate neighbors, focusing only on those where the *focal* is higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes = adjlist_wealth.sort_values('diff', ascending=False).head(10)\n",
    "extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-emission",
   "metadata": {},
   "source": [
    "Thus, we see that observation $473$ appears often on the the `focal` side, suggesting it's quite distinct from its nearby polygons. We also see observation $343$ often in the `focal` column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-florida",
   "metadata": {},
   "source": [
    "To verify whether these differences are truly beyond the pale, we can use a map randomization strategy. In this case, we shuffle the map and compute *new* `diff` columns. But, this time, `diff` represents the difference between random neighbors, rather than the neighbor structure we did observe, encoded in our Rook contiguity matrix. Using many `diff` vectors, we can find the observed differences which tend to be much larger than those encountered in randomly-drawn maps of household income.\n",
    "\n",
    "So, to start, we can construct many random `diff` vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 1000\n",
    "simulated_diffs = numpy.empty((len(adjlist), n_simulations))\n",
    "for i in range(n_simulations):\n",
    "    median_hh_focal = adjlist_wealth['median_hh_income_focal'].values\n",
    "    random_wealth = san_diego_tracts[['median_hh_income']].sample(frac=1, replace=False).reset_index()\n",
    "    adjlist_random_wealth = adjlist.merge(random_wealth, left_on='focal', right_index=True)\\\n",
    "                                   .merge(random_wealth, left_on='neighbor', right_index=True, \n",
    "                                          suffixes=('_focal','_neighbor'))\n",
    "    simulated_diffs[:,i] = adjlist_random_wealth['median_hh_income_focal'] - adjlist_random_wealth['median_hh_income_neighbor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-embassy",
   "metadata": {},
   "source": [
    "After running our simulations, we get many distributions of pairwise differences in household income. Below, we can see the shroud of all of the simulated differences, shown in black, and our observed differences, shown in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-collect",
   "metadata": {
    "caption": "Differences between neighboring incomes for the observed map (orange) and maps arising from randomly-reshuffled maps (black) of tract median incomes."
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,3))\n",
    "plt.hist(adjlist_wealth['diff'], \n",
    "         color='salmon', bins=50, density=True,\n",
    "         alpha=1, linewidth=4)\n",
    "[plt.hist(simulation, histtype='step', \n",
    "         color='k', alpha=.01, linewidth=1, \n",
    "         bins=50, density=True) for simulation in simulated_diffs.T]\n",
    "plt.hist(adjlist_wealth['diff'], histtype='step', \n",
    "         edgecolor='orangered', bins=50, density=True,\n",
    "         linewidth=2)\n",
    "seaborn.despine()\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Dollar Differences ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-gibraltar",
   "metadata": {},
   "source": [
    "Again, our random distribution is much more dispersed than our observed distribution of the differences between nearby tracts. Empirically, we can pool our simulations and construct and use their quantiles to summarize how unlikely any of our *observed* differences are if neighbors' household incomes were randomly assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_diffs = simulated_diffs.flatten()\n",
    "lower, median, upper = numpy.percentile(pooled_diffs, q=(.5,50,99.5))\n",
    "outside = (adjlist_wealth['diff'] < lower) | (adjlist_wealth['diff'] > upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-orleans",
   "metadata": {},
   "source": [
    "So, despite the fact that that our observed differences are less dispersed on average, we can identify two boundaries in the data that are in the top 1% most extreme differences in neighboring household incomes across the map. These boundaries are shown in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_wealth[outside]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-saudi",
   "metadata": {},
   "source": [
    "Note that one of these, observation $473$, appears in both boundaries. This means that the observation is likely to be *outlying*, extremely unlike *all* of its neighbors. These kinds of generalized neighborhood comparisons are discussed in the subsequent chapter on Local Spatial autocorrelation.\n",
    "\n",
    "It is most helpful, though, to visualize this on a map, focusing on the two boundaries around observation $473$, shown also in the larger context of San Diego incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-aerospace",
   "metadata": {
    "caption": "The two most stark differences in median household income among San Diego tracts."
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1, 3, figsize=(18,6))\n",
    "\n",
    "# Plot tracts\n",
    "for i in range(2):\n",
    "    san_diego_tracts.plot('median_hh_income', ax=ax[i])\n",
    "\n",
    "# Zoom 1\n",
    "first_focus = san_diego_tracts.iloc[[473,157]]\n",
    "ax[0].plot(first_focus.centroid.x, first_focus.centroid.y, color='red')\n",
    "west,east,south,north = first_focus.buffer(1000).total_bounds\n",
    "ax[0].axis([west, south, east, north])\n",
    "\n",
    "# Zoom 2\n",
    "second_focus = san_diego_tracts.iloc[[473,163]]\n",
    "ax[1].plot(second_focus.centroid.x, second_focus.centroid.y, color='red')\n",
    "ax[1].axis([west, south, east, north])\n",
    "\n",
    "# Context\n",
    "san_diego_tracts.plot(facecolor=\"k\", edgecolor=\"w\", linewidth=0.5, alpha=0.5, ax=ax[2])\n",
    "contextily.add_basemap(ax[2], crs=san_diego_tracts.crs)\n",
    "area_of_focus = pandas.concat((first_focus, second_focus)).buffer(12000).total_bounds\n",
    "ax[2].plot(first_focus.centroid.x, first_focus.centroid.y, color='red')\n",
    "ax[2].plot(second_focus.centroid.x, second_focus.centroid.y, color='red')\n",
    "west, east, south, north = area_of_focus\n",
    "ax[2].axis([west, south, east, north]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-syntax",
   "metadata": {},
   "source": [
    "These are the starkest contrasts in the map, and result in the most distinctive divisions between adjacent tracts' household incomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-tunnel",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Spatial weights are central to how we *represent* spatial relationships in mathematical and computational environments. At their core, they are a \"geo-graph,\" or a network defined by the geographical relationships between observations. They form kind of a \"spatial index,\" in that they record which observations have a specific geographical relationship. Since spatial weights are fundamental to how \"proximity\" is represented in geographic data science, we will use them again and again throughout the book. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-hunter",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-transfer",
   "metadata": {},
   "source": [
    "1. Rook contiguity & Queen contiguity are two of three kinds of contiguity that are defined in terms of chess analogies. The third kind, *Bishop contiguity*, applies when two observations are considered connected when they share single vertices, but are considered *disconnected* if they share an edge. This means that observations that exhibit Queen contiguity are those that exhibit either Rook or Bishop contiguity. Using the Rook and Queen contiguity matrices we built for San Diego and the `Wsets.w_difference` function, are there any Bishop-contiguous observations in San Diego? \n",
    "\n",
    "2. Different kinds of spatial weights objects can result in very different kinds of graph structures. Considering the `cardinalities` of the Queen, Block, and the union of Queen & Block, \n",
    "    1. Which graph type has the highest average cardinality?\n",
    "    2. Which graph has more nonzero entries?\n",
    "    3. Why might this be the case?\n",
    "\n",
    "3. Graphs are considered \"connected\" when you can construct a path from any observation to every other observation. A \"disconnected\" graph has at least one node where there is no path from it to every other node. And, a \"connected component\" is a part of the graph that is connected internally, but is disconnected from another part of the graph. This is reported for every spatial weights object in its `w.n_components`. \n",
    "\n",
    "    1. How many components does the Queen Contiguity weights for San Diego have?\n",
    "    2. Using a K-nearest Neighbor Graph for San Diego tracts where $k=1$, how many connected components are there in this graph?\n",
    "    3. Increase $k$ by one until the `n_components` is 1. Make a plot of the relationship between $k$ and $n_components$. \n",
    "    4. What value of $k$ does `n_components` become 1?\n",
    "    5. How many non-zero links does this network have?\n",
    "\n",
    "4. Comparing their average cardinality and percentage of nonzero links, which graph in this chapter has the *most sparse* structure? That is, which graph is the most sparsely connected?\n",
    "\n",
    "5. In this chapter, we worked with regular *square* lattices using the `lat2W` function. In the same manner, the `hexLat2W` function can generate *hexagonal regular lattices*. For lattices of size (3,3), (6,6), and (9,9) for Rook & Queen `lat2W`, as well as for `hexLat2W`:\n",
    "\n",
    "    1. examine the average cardinality. Does `lat2W` or `hexLat2W` have higher average cardinality? \n",
    "    2. Further, make a histogram of the cardinalities. Which type of lattice has higher variation in its number of neighbors? \n",
    "    3. Why is there no `rook=True` option in `hexLat2W`, as there is in `lat2W`?\n",
    "\n",
    "6. The *Voronoi diagram* is a common method to construct polygons from a point dataset. A Voronoi diagram is built up from *Voronoi cells*, each of which contains the area that is closer to its source point than any other source point in the diagram. Further, the Queen contiguity graph for a *Voronoi diagram* obeys a number of useful properties, since it is the *Delaunay Triangulation* of a set of points. \n",
    "    1. Using the following code, build and plot the Voronoi diagram for the *centroids* of Mexican states, with the states and their centroids overlayed:\n",
    "    ```python\n",
    "    from pysal.lib.weights.distance import get_points_array\n",
    "    from pysal.lib.cg import voronoi_frames\n",
    "    \n",
    "    centroid_coordinates = get_points_array(mx.centroid)\n",
    "    cells, centers = voronoi_frames(centroid_coordinates)\n",
    "    \n",
    "    ax = cells.plot(facecolor='none', edgecolor='k')\n",
    "    mx.plot(ax=ax, edgecolor='red', facecolor='whitesmoke', alpha=.5)\n",
    "    mx.centroid.plot(ax=ax,color='red', alpha=.5, markersize=10)\n",
    "    ax.axis(mx.total_bounds[[0,2,1,3]])\n",
    "    plt.show()\n",
    "    ```\n",
    "    2. Using the `weights.Voronoi` function, build the Voronoi weights for the Mexico states data.\n",
    "    3. Compare the connections in the Voronoi and Queen weights for the Mexico states data. Which form is more connected? \n",
    "    4. Make a plot of the Queen contiguity and Voronoi contiguity graphs to compare them visually, like we did with the block weights & Queen weights. How do the two graphs compare in terms of the length of their links and how they connect the Mexican states?\n",
    "    5. Using `weights.set_operations`, find any links that are in the Voronoi contiguity graph, but not in the Queen contiguity graph. Alternatively, find any links that are in the Queen contiguity graph, but not the Voronoi contiguity graph. \n",
    "\n",
    "7. Interoperability is important for the Python scientific stack. Thanks to standardization around the `numpy` array and the `scipy.sparse` array data structures, it is simple and computationally-easy to convert objects from one representation to another:\n",
    "    1. Using `w.to_networkx()`, convert the Mexico Regions Queen+Block weights matrix to a `networkx` graph. Compute the Eigenvector Centrality of that new object using `networkx.eigenvector_centrality`\n",
    "    2. Using `w.sparse`, compute the number of connected components in the Mexico Regions Block weights matrix using the `connected_components` function in `scipy.sparse.csgraph`. \n",
    "    3. Using `w.sparse`, compute the all-pairs shortest path matrix in the Mexico Queen weights matrix using the `shortest_path` function in `scipy.sparse.csgraph`. \n",
    "    \n",
    "8. While every node in a $k$-nearest neighbor graph has 5 neighbors, there is a conceptual difference between *in-degree* and *out-degree* of nodes in a graph. The *out-degree* of a node is the number of outgoing links from a node; for a K-Nearest Neighbor graph, this is $k$ for every variable. The *in-degree* of a node in a graph is the number of *incoming* links to that node; for a K-Nearest Neighbor graph, this is the number of other observations that pick the target as their nearest neighbor. The *in-degree* of a node in the K-Nearest Neighbor graph can provide a measure of *hubbiness*, or how central a node is to other nodes. \n",
    "    1. Using the San Diego Tracts data, build a $k=6$ nearest neighbor weight and call it `knn_6`. \n",
    "    2. Verify that the $k=6$ by taking the row sum over the weights matrix in `knn_6.sparse`.\n",
    "    3. Compute the in-degree of each observation by taking the *column sum* over the weights matrix in `knn_6.sparse`, and divide by 6, the out-degree for all observations. \n",
    "    4. Make a histogram of the in-degrees for the $k=6$ weights. How evenly-distributed is the distribution of in-degrees?\n",
    "    5. Make a new histogram of the in-degree standardized by the out-degree when $k=26$. Does hubbiness reduce when increasing the number of $k$-nearest neighbors?\n",
    "\n",
    "9. Sometimes, graphs are not simple to construct. For the `san_diego_neighborhoods` dataset:\n",
    "    1. Build the Queen contiguity weights, and plot the graph on top of the neighborhoods themselves. How many connected components does this Queen contiguity graph have? \n",
    "    2. Build the K-Nearest Neighbor graph for the default, $k=2$. How many connected components does this K-Nearest Neighbor graph have? \n",
    "    3. What is the smallest $k$ that you can find for the K-Nearest Neighbor graph to be fully-connected?\n",
    "    4. In graph theory, a link whose *removal* will increase the number of connected components in a graph is called a *bridge*. In the fully-connected KNN graph with the smallest $k$, how many bridges are there between the north and south components? *(hint: use the plotting functionality)*\n",
    "    5. What are the next two values of $k$ required for there to be an *additional* bridge at that $k$?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
