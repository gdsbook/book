
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Clustering &amp; Regionalization &#8212; Geographic Data Science with Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://geographicdata.science/book/notebooks/10_clustering_and_regionalization.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Spatial Regression" href="11_regression.html" />
    <link rel="prev" title="Spatial Inequality" href="09_spatial_inequality.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Geographic Data Science with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Home
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preface
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_toc.html">
   Table of Contents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   References
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part I - Building Blocks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro_part_i.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_geo_thinking.html">
   Geographic thinking for data scientists
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_geospatial_computational_environment.html">
   Geospatial Computational Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_spatial_data.html">
   Spatial Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_spatial_weights.html">
   Spatial Weights
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part II - Spatial Data Analysis
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro_part_ii.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_choropleth.html">
   Choropleth Mapping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_spatial_autocorrelation.html">
   Global Spatial Autocorrelation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_local_autocorrelation.html">
   Local Spatial Autocorrelation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_point_pattern_analysis.html">
   Point Pattern Analysis
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part III - Advanced Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro_part_ii.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_spatial_inequality.html">
   Spatial Inequality
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Clustering &amp; Regionalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_regression.html">
   Spatial Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_feature_engineering.html">
   Spatial Feature Engineering
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../data/README.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/airbnb/regression_cleaning.html">
   AirBnb
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/airports/airports_cleaning.html">
   Airports
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/brexit/brexit_cleaning.html">
   Brexit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/countries/countries_cleaning.html">
   Countries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/ghsl/build_ghsl_extract.html">
   GHSL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/h3_grid/build_sd_h3_grid.html">
   H3 Grid
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/mexico/README.html">
   Mexico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/nasadem/build_nasadem_sd.html">
   NASA DEM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/sandiego/sandiego_tracts_cleaning.html">
   San Diego Tracts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/texas/README.html">
   Texas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/tokyo/tokyo_cleaning.html">
   Tokyo Photographs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/us_county_income/README.html">
   US County Income 1969-2017
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/10_clustering_and_regionalization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/gdsbook/book/master?urlpath=lab/tree/notebooks/10_clustering_and_regionalization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/gdsbook/book/blob/master/notebooks/10_clustering_and_regionalization.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#geodemographic-clusters-in-san-diego-census-tracts">
   Geodemographic Clusters in San Diego Census Tracts
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means">
     K-means
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spatial-distribution-of-clusters">
     Spatial Distribution of Clusters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-analysis-of-the-cluster-map">
     Statistical Analysis of the Cluster Map
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-clustering">
   Hierarchical Clustering
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spatially-constrained-hierarchical-clustering">
   Spatially Constrained Hierarchical Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contiguity-constraint">
     Contiguity constraint
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#changing-the-spatial-constraint">
     Changing the spatial constraint
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="clustering-regionalization">
<h1>Clustering &amp; Regionalization<a class="headerlink" href="#clustering-regionalization" title="Permalink to this headline">¶</a></h1>
<!--
**NOTE**: parts of this notebook have been
borrowed from [GDS'17 - Lab
6](http://darribas.org/gds17/content/labs/lab_06.html)
--><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">esda.moran</span> <span class="kn">import</span> <span class="n">Moran</span>
<span class="kn">from</span> <span class="nn">libpysal.weights</span> <span class="kn">import</span> <span class="n">Queen</span><span class="p">,</span> <span class="n">KNN</span>
<span class="kn">import</span> <span class="nn">seaborn</span> 
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">geopandas</span> 
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The world’s hardest questions are complex and multi-faceted.
Effective methods to learn from data should recognize this. Many questions
and challenges are inherently multidimensional; they are affected, shaped, and
defined by many different components all acting simultaneously. In statistical
terms, these processes are called <em>multivariate processes</em>, as opposed to
<em>univariate processes</em>, where only a single variable acts at once.
Clustering is a fundamental method of geographical analysis that draws insights
from large, complex multivariate processes. It works by finding similarities among the
many dimensions in a multivariate process, condensing them down into a simpler representation.
Thus, through clustering, a complex and difficult to understand process is recast into a simpler one
that even non-technical audiences can work with.</p>
<p>Clustering as we discuss it in this chapter borrows heavily from unsupervised statistical learning <span id="id1">[<a class="reference internal" href="references.html#id5"><span>FHT+01</span></a>]</span>.
Often, clustering involves sorting observations into groups without any prior idea on what
the groups are (or, in machine learning jargon, without any labels, hence the <em>unsupervised</em> nature).
These groups are delineated so that members of a group should be more
similar to one another than they are to members of a different group.
Each group is referred to as a <em>cluster</em> while the process of assigning
objects to groups is known as <em>clustering</em>. If done well, these clusters can be
characterized by their <em>profile</em>, a simple summary of what members of a group
are like in terms of the original multivariate phenomenon.</p>
<p>Since a good cluster is more
similar internally than it is to any other cluster, these cluster-level profiles
provide a convenient shorthand to describe the original complex multivariate phenomenon
we are interested in.
Observations in one group may have consistently high
scores on some traits but low scores on others.
The analyst only needs to look at the profile of a cluster in order to get a
good sense of what all the observations in that cluster are like, instead of
having to consider all of the complexities of the original multivariate process at once.
Throughout data science, and particularly in geographic data science,
clustering is widely used to provide insights on the
geographic structure of complex multivariate spatial data.</p>
<p>In the context of explicitly spatial questions, a related concept, the <em>region</em>,
is also instrumental. A <em>region</em> is similar to a <em>cluster</em>, in the sense that
all members of a region have been grouped together, and the region should provide
a shorthand for the original data.
For a region to be analytically useful, its members also should
display stronger similarity to each other than they do to the members of other regions.
However, regions are more complex than clusters because they combine this
similarity in profile with additional information about the location of their members.
In short, regions are like clusters (since they have a consistent profile) where all its members
are geographically consistent.</p>
<p>The process of creating regions is called regionalization <span id="id2">[<a class="reference internal" href="references.html#id4"><span>DRSurinach07</span></a>]</span>.
A regionalization is a special kind of clustering where the objective is
to group observations which are similar in their statistical attributes,
but also in their spatial location. In this sense, regionalization embeds the same
logic as standard clustering techniques, but also applies a series of
spatial and/or geographical constraints. Often, these
constraints relate to connectivity: two candidates can only be grouped together in the
same region if there exists a path from one member to another member
that never leaves the region. These paths often model the spatial relationships
in the data, such as contiguity or proximity. However, connectivity does not
always need to hold for all regions, and in certain contexts it makes
sense to relax connectivity or to impose different types of spatial constraints.</p>
<p>In this chapter we consider clustering techniques and regionalization methods which will
allow us to do exactly that. In the process, we will explore the socioeconomic
characteristics of neighborhoods in San Diego. We will extract common patterns from the
cloud of multidimensional data that the Census Bureau produces about small areas
through the American Community Survey. We begin with an exploration of the
multivariate nature of our dataset by suggesting some ways to examine the
statistical and spatial distribution before carrying out any
clustering. Focusing on the individual variables, as well as their pairwise
associations, can help guide the subsequent application of clusterings or regionalizations.</p>
<p>We then consider geodemographic approaches to clustering—the application
of multivariate clustering to spatially referenced demographic data.
Two popular clustering algorithms are employed: k-means and Ward’s hierarchical method.
As we will see, mapping the spatial distribution of the resulting clusters
reveals interesting insights on the socioeconomic structure of the San Diego
metropolitan area. We also see that in many cases, clusters are spatially
fragmented. That is, a cluster may actually consist of different areas that are not
spatially connected. Indeed, some clusters will have their members strewn all over the map.
This will illustrate why connectivity might be important when building insight
about spatial data, since these clusters will not at all provide intelligible regions.
With this insight in mind, we will move on to regionalization, exploring different approaches that
incorporate geographical constraints into the exploration of the social structure of San Diego.
Applying a regionalization approach is not always necessarily preferable but it can provide
additional insights into the spatial structure of the multivariate statistical relationships
that traditional clustering is unable to articulate.</p>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>The dataset we will use in this chapter comes from the American Community Survey
(ACS). In particular, we examine data at the Census Tract level in San Diego,
California in 2017. Let us begin by reading in the data as a GeoDataFrame and
exploring the attribute names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read file</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">geopandas</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="s1">&#39;../data/sandiego/sandiego_tracts.gpkg&#39;</span><span class="p">)</span>
<span class="c1"># Print column names</span>
<span class="n">db</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;GEOID&#39;, &#39;median_age&#39;, &#39;total_pop&#39;, &#39;total_pop_white&#39;, &#39;tt_work&#39;,
       &#39;hh_total&#39;, &#39;hh_female&#39;, &#39;total_bachelor&#39;, &#39;median_hh_income&#39;,
       &#39;income_gini&#39;, &#39;total_housing_units&#39;, &#39;total_rented&#39;, &#39;median_no_rooms&#39;,
       &#39;median_house_value&#39;, &#39;NAME&#39;, &#39;state&#39;, &#39;county&#39;, &#39;tract&#39;, &#39;area_sqm&#39;,
       &#39;pct_rented&#39;, &#39;pct_hh_female&#39;, &#39;pct_bachelor&#39;, &#39;pct_white&#39;, &#39;sub_30&#39;,
       &#39;geometry&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<p>To make things easier later on, let us collect the variables we will use to
characterize Census tracts. These variables capture different aspects of the
socioeconomic reality of each area and, taken together, provide a comprehensive
characterization of San Diego as a whole:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cluster_variables</span> <span class="o">=</span>  <span class="p">[</span>
    <span class="s1">&#39;median_house_value&#39;</span><span class="p">,</span> <span class="c1"># Median house value</span>
    <span class="s1">&#39;pct_white&#39;</span><span class="p">,</span>          <span class="c1"># Percent of tract population that is white</span>
    <span class="s1">&#39;pct_rented&#39;</span><span class="p">,</span>         <span class="c1"># Percent of households that are rented</span>
    <span class="s1">&#39;pct_hh_female&#39;</span><span class="p">,</span>      <span class="c1"># Percent of female-led households </span>
    <span class="s1">&#39;pct_bachelor&#39;</span><span class="p">,</span>       <span class="c1"># Percent of tract population with a Bachelors degree</span>
    <span class="s1">&#39;median_no_rooms&#39;</span><span class="p">,</span>    <span class="c1"># Median number of rooms in the tract&#39;s households</span>
    <span class="s1">&#39;income_gini&#39;</span><span class="p">,</span>        <span class="c1"># Gini index measuring tract wealth inequality</span>
    <span class="s1">&#39;median_age&#39;</span><span class="p">,</span>         <span class="c1"># Median age of tract population</span>
    <span class="s1">&#39;tt_work&#39;</span>             <span class="c1"># Travel time to work </span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s start building up our understanding of this
dataset through both visual and summary statistical measures.
The first stop is considering the spatial distribution of each variable alone.
This will help us draw a picture of the multi-faceted view of the tracts we
want to capture with our clustering. Let’s use (quantile) choropleth maps for
each attribute and compare them side-by-side:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="c1"># Make the axes accessible with single indexing</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="c1"># Start a loop over all the variables of interest</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cluster_variables</span><span class="p">):</span>
    <span class="c1"># select the axis where the map will go</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="c1"># Plot the map</span>
    <span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">column</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> 
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">scheme</span><span class="o">=</span><span class="s1">&#39;Quantiles&#39;</span><span class="p">,</span> 
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdPu&#39;</span>
    <span class="p">)</span>
    <span class="c1"># Remove axis clutter</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="c1"># Set the axis title to the name of variable being plotted</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
<span class="c1"># Display the figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_8_0.png" src="../_images/10_clustering_and_regionalization_8_0.png" />
</div>
</div>
<p>Several visual patterns jump out from the maps, revealing both commonalities as
well as differences across the spatial distributions of the individual variables.
Several variables tend to increase in value from the east to the west
(<code class="docutils literal notranslate"><span class="pre">pct_rented</span></code>, <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code>, <code class="docutils literal notranslate"><span class="pre">median_no_rooms</span></code>, and <code class="docutils literal notranslate"><span class="pre">tt_work</span></code>) while others
have a spatial trend in the opposite direction (<code class="docutils literal notranslate"><span class="pre">pct_white</span></code>, <code class="docutils literal notranslate"><span class="pre">pct_hh_female</span></code>,
<code class="docutils literal notranslate"><span class="pre">pct_bachelor</span></code>, <code class="docutils literal notranslate"><span class="pre">median_age</span></code>). This will help show the strengths of clustering;
when variables have
different spatial distributions, each variable contributes distinct
information to the profiles of each cluster. However, if all variables display very similar
spatial patterns, the amount of useful information across the maps is
actually smaller than it appears, so cluster profiles may be much less useful as well.
It is also important to consider whether the variables display any
spatial autocorrelation, as this will affect the spatial structure of the
resulting clusters.</p>
<p>Recall from <a class="reference internal" href="06_spatial_autocorrelation.html"><span class="doc std std-doc">Chapter 6</span></a> that Moran’s I is a commonly used
measure for global spatial autocorrelation. We can use it to formalise some of the
intuitions built from the maps. Recall from earlier in the book that we will need
to represent the spatial configuration of the data points through a spatial weights
matrix. We will start with queen contiguity:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">Queen</span><span class="o">.</span><span class="n">from_dataframe</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s calculate Moran’s I for the variables being used. This will measure
the extent to which each variable contains spatial structure:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="c1"># Calculate Moran&#39;s I for each variable</span>
<span class="n">mi_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">Moran</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">variable</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">cluster_variables</span><span class="p">]</span>
<span class="c1"># Structure results as a list of tuples</span>
<span class="n">mi_results</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">I</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">p_sim</span><span class="p">)</span> <span class="k">for</span> <span class="n">variable</span><span class="p">,</span><span class="n">res</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cluster_variables</span><span class="p">,</span> <span class="n">mi_results</span><span class="p">)</span>
<span class="p">]</span>
<span class="c1"># Display on table</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">mi_results</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s2">&quot;Moran&#39;s I&quot;</span><span class="p">,</span> <span class="s1">&#39;P-value&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Variable&#39;</span><span class="p">)</span>
<span class="n">table</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Moran's I</th>
      <th>P-value</th>
    </tr>
    <tr>
      <th>Variable</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>median_house_value</th>
      <td>0.646618</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.602079</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_rented</th>
      <td>0.451372</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_hh_female</th>
      <td>0.282239</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.433082</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>median_no_rooms</th>
      <td>0.538996</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>income_gini</th>
      <td>0.295064</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>median_age</th>
      <td>0.381440</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>tt_work</th>
      <td>0.102748</td>
      <td>0.001</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Each of the variables displays significant positive spatial autocorrelation,
suggesting clear spatial structure in the socioeconomic geography of San
Diego. This means it is likely the clusters we find will have
a non random spatial distribution.</p>
<p>Spatial autocorrelation only describes relationships between observations for a
single attribute at a time.
So, the fact that all of the clustering variables are positively autocorrelated does not
say much about how attributes co-vary over space. To explore cross-attribute relationships,
we need to consider the spatial correlation between variables. We will take our first dip
in this direction exploring the bivariate correlation in the maps of covariates themselves.
This would mean that we would be comparing each pair of chroplets to look for associations
and differences. Given there are nine attributes, there are 36 pairs of maps that must be
compared. These are too many maps to process visually. Instead, we focus directly
on the bivariate relationhips between each pair of attributes, devoid for now of geography
and use a scatterplot matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;reg&#39;</span><span class="p">,</span> <span class="n">diag_kind</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_14_0.png" src="../_images/10_clustering_and_regionalization_14_0.png" />
</div>
</div>
<p>Two different types of plots are contained in the scatterplot matrix. On the
diagonal are the density functions for the nine attributes. These allow for an
inspection of the univariate distribution of the values for each attribute.
Examining these we see that our selection of variables includes some that are
negatively skewed (<code class="docutils literal notranslate"><span class="pre">pct_white</span></code> and <code class="docutils literal notranslate"><span class="pre">pct_hh_female</span></code>) as well as positively skewed
(<code class="docutils literal notranslate"><span class="pre">median_house_value</span></code>, <code class="docutils literal notranslate"><span class="pre">pct_bachelor</span></code>, and <code class="docutils literal notranslate"><span class="pre">tt_work</span></code>).</p>
<p>The second type of visualization lies in the off-diagonal cells of the matrix;
these are bi-variate scatterplots. Each cell shows the association between one
pair of variables. Several of these cells indicate positive linear
associations (<code class="docutils literal notranslate"><span class="pre">median_age</span></code> Vs. <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code>, <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code> Vs. <code class="docutils literal notranslate"><span class="pre">median_no_rooms</span></code>)
while other cells display negative correlation (<code class="docutils literal notranslate"><span class="pre">median_house_value</span></code> Vs. <code class="docutils literal notranslate"><span class="pre">pct_rented</span></code>,
<code class="docutils literal notranslate"><span class="pre">median_no_rooms</span></code> Vs. <code class="docutils literal notranslate"><span class="pre">pct_rented</span></code>, and <code class="docutils literal notranslate"><span class="pre">median_age</span></code> Vs. <code class="docutils literal notranslate"><span class="pre">pct_rented</span></code>). The one variable
that tends to have consistently weak association with the other variables is
<code class="docutils literal notranslate"><span class="pre">tt_work</span></code>, and in part this appears to reflect its rather concentrated
distribution as seen on the lower right diagonal corner cell.</p>
<p>Exploring univariate and bivariate relationships is a good first step into building
a fully multivariate understanding of a dataset. To take it to the next level, we would
want to know to what extent these pair-wise relationships hold across different attributes,
and whether there are patterns in the “location” of observations within the scatter plots.
For example, do nearby dots in each scatterplot of the matrix represent the <em>same</em> observations?
This type of questions are exactly what clustering helps us explore.</p>
</div>
<div class="section" id="geodemographic-clusters-in-san-diego-census-tracts">
<h2>Geodemographic Clusters in San Diego Census Tracts<a class="headerlink" href="#geodemographic-clusters-in-san-diego-census-tracts" title="Permalink to this headline">¶</a></h2>
<p>Geodemographic analysis is a form of multivariate
clustering where the observations represent geographical areas <span id="id3">[<a class="reference internal" href="references.html#id3"><span>WB18</span></a>]</span>. The output
of these clusterings is nearly always mapped. Altogether, these methods use
multivariate clustering algorithms to construct a known number of
clusters (<span class="math notranslate nohighlight">\(k\)</span>), where the number of clusters is typically much smaller than the
number of observations to be clustered. Each cluster is given a unique label,
and these labels are mapped. Using the clusters’ profile and label, the map of
labels can be interpreted to get a sense of the spatial distribution of
socio-demographic traits. The power of (geodemographic) clustering comes
from taking statistical variation across several dimensions and compressing it
into a single categorical one that we can visualize through a map. To
demonstrate the variety of approaches in clustering, we will show two
distinct but very popular clustering algorithms: k-means and Ward’s hierarchical method.</p>
<div class="section" id="k-means">
<h3>K-means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h3>
<p>K-means is probably the most widely used approach to
cluster a dataset. The algorithm groups observations into a
pre-specified number of clusters so that that each observation is
closer to the mean of its own cluster than it is to the mean of any other cluster.
The k-means problem is solved by iterating between an assignment step and an update step.
First, all observations are randomly assigned one of the <span class="math notranslate nohighlight">\(k\)</span> labels. Next, the
multivariate mean over all covariates is calculated for each of the clusters.
Then, each observation is reassigned to the cluster with the closest mean.
If the observation is already assigned to the cluster whose mean it is closest to,
the observation remains in that cluster. This assignment-update process continues
until no further reassignments are necessary.</p>
<p>The nature of this algorithm requires us to select the number of clusters we
want to create. The right number of clusters is unknown in practice. For
illustration, we will use <span class="math notranslate nohighlight">\(k=5\)</span> in the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> implementation from
<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialise KMeans instance</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</pre></div>
</div>
</div>
</div>
<p>This illustration will also be useful as virtually every algorithm in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>,
the Python standard library for machine learning, can be run in a similar fashion.
To proceed, we first create a <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> clusterer object that contains the description of
all the parameters the algorithm needs (in this case, only the number of clusters):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialise KMeans instance</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we call the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to compute the algorithm specified in <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> to the variables
we are interested in <code class="docutils literal notranslate"><span class="pre">db</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="c1"># Run K-Means algorithm</span>
<span class="n">k5cls</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now that the clusters have been assigned, we can examine the label vector, which
records the cluster to which each observation is assigned:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k5cls</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 2, 1, 2,
       3, 1, 4, 0, 1, 1, 1, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 4, 1, 4, 4, 1,
       4, 4, 4, 0, 1, 1, 4, 1, 1, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1,
       4, 4, 0, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1, 4, 0, 4, 0, 4, 4, 0, 1, 1,
       1, 0, 3, 3, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 4, 0, 1, 1, 1, 0, 1,
       1, 0, 0, 4, 1, 4, 2, 4, 0, 4, 4, 4, 4, 1, 4, 4, 1, 0, 4, 4, 4, 4,
       4, 1, 1, 1, 1, 4, 4, 1, 0, 4, 4, 4, 1, 4, 4, 0, 0, 0, 1, 2, 2, 2,
       4, 4, 4, 4, 4, 2, 0, 0, 0, 1, 1, 1, 0, 1, 2, 1, 1, 2, 2, 0, 3, 1,
       1, 1, 1, 0, 4, 1, 4, 4, 4, 4, 1, 1, 4, 4, 0, 1, 4, 4, 4, 1, 1, 1,
       1, 4, 4, 4, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 4, 1, 4, 1,
       1, 4, 1, 4, 1, 1, 0, 4, 0, 4, 0, 3, 4, 1, 2, 0, 4, 1, 1, 4, 2, 1,
       4, 4, 1, 0, 0, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 0, 0, 1, 1, 4,
       4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 4, 0, 0, 4, 4, 1, 4, 4, 4, 1, 0, 4,
       4, 1, 4, 4, 4, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4, 4, 0, 4, 1, 1, 1, 1,
       1, 4, 1, 4, 4, 3, 4, 0, 1, 0, 0, 0, 0, 0, 1, 0, 4, 1, 1, 1, 1, 1,
       4, 0, 0, 2, 4, 1, 1, 4, 1, 1, 1, 0, 4, 0, 4, 4, 0, 4, 4, 4, 0, 1,
       4, 4, 4, 4, 4, 1, 4, 1, 4, 4, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 4,
       1, 1, 4, 0, 4, 1, 1, 1, 4, 0, 4, 1, 4, 4, 1, 0, 2, 0, 4, 3, 0, 0,
       0, 0, 1, 4, 1, 0, 1, 1, 1, 3, 0, 0, 1, 4, 4, 4, 1, 4, 4, 4, 4, 4,
       1, 1, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 1, 0, 4, 4, 1, 4, 1, 4, 1, 4,
       4, 0, 4, 4, 4, 4, 1, 4, 4, 4, 1, 1, 4, 1, 0, 4, 1, 4, 4, 1, 1, 1,
       4, 4, 2, 4, 1, 4, 4, 1, 0, 3, 0, 2, 2, 4, 0, 1, 1, 1, 4, 0, 0, 4,
       1, 4, 0, 4, 1, 1, 4, 1, 4, 4, 2, 0, 0, 4, 2, 4, 4, 1, 1, 4, 4, 4,
       4, 4, 1, 4, 0, 4, 4, 0, 1, 4, 4, 1, 4, 1, 4, 4, 4, 4, 1, 1, 0, 1,
       0, 1, 1, 4, 4, 1, 1, 4, 1, 4, 4, 4, 1, 0, 1, 1, 4, 1, 1, 4, 1, 0,
       0, 0, 4, 0, 0, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 2, 1, 1, 0, 4, 1,
       1, 0, 1, 1, 1, 4, 1, 1, 4, 1, 1, 4, 0, 4, 4, 4, 0, 4, 4, 4, 4, 1,
       4, 1, 4, 4, 4, 1, 1, 4, 1, 4, 1, 1, 0, 0, 4, 4, 1, 1, 1, 1, 1, 1,
       4, 0, 0, 4, 0, 4, 1, 1, 1, 1, 1, 4], dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this case, the second and third observations are assigned to cluster 4, while
the fourth and fifth observations have been placed in cluster 0. It is important
to note that the integer labels should be viewed as denoting membership only —
the numerical differences between the values for the labels are meaningless.
The profiles of the various clusters must be further explored by looking
at the values of each dimension.</p>
<p>But, before we do that, let’s make a map.</p>
</div>
<div class="section" id="spatial-distribution-of-clusters">
<h3>Spatial Distribution of Clusters<a class="headerlink" href="#spatial-distribution-of-clusters" title="Permalink to this headline">¶</a></h3>
<p>Having obtained the cluster labels, we can display the spatial
distribution of the clusters by using the labels as the categories in a
choropleth map. This allows us to quickly grasp any sort of spatial pattern the
clusters might have. Since clusters represent areas with similar
characteristics, mapping their labels allows to see to what extent similar areas tend
to have similar locations.
Thus, this gives us one map that incorporates the information of from all nine covariates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assign labels into a column</span>
<span class="n">db</span><span class="p">[</span><span class="s1">&#39;k5cls&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">k5cls</span><span class="o">.</span><span class="n">labels_</span>
<span class="c1"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c1"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">column</span><span class="o">=</span><span class="s1">&#39;k5cls&#39;</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
<span class="p">)</span>
<span class="c1"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c1"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_24_0.png" src="../_images/10_clustering_and_regionalization_24_0.png" />
</div>
</div>
<p>The map provides a useful view of the clustering results; it allows for
a visual inspection of the extent to which Tobler’s first law of geography is
reflected in the multivariate clusters. Recall that the law implies that nearby
tracts should be more similar to one another than tracts that are geographically
more distant from each other. We can see evidence of this in
our cluster map, since clumps of tracts with the same color emerge. However, this
visual inspection is obscured by the complexity of the underlying spatial
units. Our eyes are drawn to the larger polygons in the eastern part of the
county, giving the impression that cluster 1 is the dominant cluster. While this
seems to be true in terms of land area (and we will verify this below), there is
more to the cluster pattern than this. Because the tract polygons are all
different sizes and shapes, we cannot solely rely on our eyes to interpret
the spatial distribution of clusters.</p>
</div>
<div class="section" id="statistical-analysis-of-the-cluster-map">
<h3>Statistical Analysis of the Cluster Map<a class="headerlink" href="#statistical-analysis-of-the-cluster-map" title="Permalink to this headline">¶</a></h3>
<p>To complement the geovisualization of the clusters, we can explore the
statistical properties of the cluster map. This process allows us to delve
into what observations are part of each cluster and what their
characteristics are.
This gives us the profile of each cluster so we can interpret the meaning of the
labels we’ve obtained. We can start, for example, by
considering cardinality, or the count of observations in each cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Group data table by cluster label and count observations</span>
<span class="n">k5sizes</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;k5cls&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">k5sizes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>k5cls
0    102
1    240
2     25
3      9
4    252
dtype: int64
</pre></div>
</div>
</div>
</div>
<p>And we can get a visual representation of cardinality as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">k5sizes</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_28_0.png" src="../_images/10_clustering_and_regionalization_28_0.png" />
</div>
</div>
<p>There are substantial differences in the sizes of the five clusters, with two very
large clusters (0, 3), one medium sized cluster (2), and two small clusters (1,
4). Cluster 3 is the largest when measured by the number of assigned tracts.
This confirms our intuition from the map above, where we got the visual impression
that tracts in cluster 3 seemed to have the largest area. Let’s see if this is
the case. One way to do so involves using the <code class="docutils literal notranslate"><span class="pre">dissolve</span></code> operation in <code class="docutils literal notranslate"><span class="pre">geopandas</span></code>, which
combines all tracts belonging to each cluster into a single
polygon object. After we have dissolved all the members of the clusters,
we report the total land area of the cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dissolve areas by Cluster, aggregate by summing, and keep column for area</span>
<span class="n">areas</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">dissolve</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;k5cls&#39;</span><span class="p">,</span> <span class="n">aggfunc</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)[</span><span class="s1">&#39;area_sqm&#39;</span><span class="p">]</span>
<span class="n">areas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>k5cls
0     639.178767
1    4137.809886
2     245.345027
3      63.002389
4    6636.436573
Name: area_sqm, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>And, to show this visually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">areas</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_32_0.png" src="../_images/10_clustering_and_regionalization_32_0.png" />
</div>
</div>
<p>Our visual impression is confirmed: cluster 3 contains tracts that
together comprise 6,636 square kilometers (approximately 2562 square miles),
which accounts for over half of the total land area in the county:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">areas</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">areas</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0053748175341845955
</pre></div>
</div>
</div>
</div>
<p>Let’s move on to build the profiles for each cluster. Again, the profiles is what
provides the conceptual shorthand, moving from the arbitrary label to a meaningful
collection of observations with similar attributes. To build a basic profile, we can
compute the means of each of the attributes in every cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Group table by cluster label, keep the variables used </span>
<span class="c1"># for clustering, and obtain their mean</span>
<span class="n">k5means</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;k5cls&#39;</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># Transpose the table and print it rounding each value</span>
<span class="c1"># to three decimals</span>
<span class="n">k5means</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>k5cls</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>median_house_value</th>
      <td>736881.373</td>
      <td>500787.575</td>
      <td>1168004.000</td>
      <td>1876867.222</td>
      <td>326728.968</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.805</td>
      <td>0.722</td>
      <td>0.841</td>
      <td>0.909</td>
      <td>0.658</td>
    </tr>
    <tr>
      <th>pct_rented</th>
      <td>0.325</td>
      <td>0.406</td>
      <td>0.288</td>
      <td>0.251</td>
      <td>0.512</td>
    </tr>
    <tr>
      <th>pct_hh_female</th>
      <td>0.104</td>
      <td>0.102</td>
      <td>0.107</td>
      <td>0.117</td>
      <td>0.106</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.004</td>
      <td>0.010</td>
      <td>0.002</td>
      <td>0.002</td>
      <td>0.020</td>
    </tr>
    <tr>
      <th>median_no_rooms</th>
      <td>5.733</td>
      <td>5.294</td>
      <td>6.020</td>
      <td>6.422</td>
      <td>4.627</td>
    </tr>
    <tr>
      <th>income_gini</th>
      <td>0.430</td>
      <td>0.401</td>
      <td>0.474</td>
      <td>0.520</td>
      <td>0.402</td>
    </tr>
    <tr>
      <th>median_age</th>
      <td>41.600</td>
      <td>37.211</td>
      <td>44.436</td>
      <td>50.544</td>
      <td>34.246</td>
    </tr>
    <tr>
      <th>tt_work</th>
      <td>2336.284</td>
      <td>2539.429</td>
      <td>2182.000</td>
      <td>1237.778</td>
      <td>2187.754</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We see that cluster 4, for example, is composed of tracts that have
the highest average <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code>, and also the highest level of inequality
(<code class="docutils literal notranslate"><span class="pre">income_gini</span></code>); and cluster 4 contains an older population (<code class="docutils literal notranslate"><span class="pre">median_age</span></code>)
who tend to live in housing units with more rooms (<code class="docutils literal notranslate"><span class="pre">median_no_rooms</span></code>).
Average values, however, can hide a great deal of detail and, in some cases,
give wrong impressions about the type of data distribution they represent. To
obtain more detailed profiles, we can use the <code class="docutils literal notranslate"><span class="pre">describe</span></code> command in <code class="docutils literal notranslate"><span class="pre">pandas</span></code>,
after grouping our observations by their clusters:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Group table by cluster label, keep the variables used </span>
<span class="c1"># for clustering, and obtain their descriptive summary</span>
<span class="n">k5desc</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;k5cls&#39;</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
<span class="c1"># Loop over each cluster and print a table with descriptives</span>
<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">k5desc</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\t</span><span class="s1">---------</span><span class="se">\n\t</span><span class="s1">Cluster </span><span class="si">%i</span><span class="s1">&#39;</span><span class="o">%</span><span class="k">cluster</span>)
    <span class="nb">print</span><span class="p">(</span><span class="n">k5desc</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span><span class="o">.</span><span class="n">unstack</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>	---------
	Cluster 0
                    count           mean           std            min  \
median_house_value  102.0  736881.372549  87489.733180  621600.000000   
pct_white           102.0       0.805041      0.126212       0.476693   
pct_rented          102.0       0.325356      0.205539       0.019410   
pct_hh_female       102.0       0.103719      0.024251       0.022453   
pct_bachelor        102.0       0.004370      0.006314       0.000000   
median_no_rooms     102.0       5.733333      1.259761       2.300000   
income_gini         102.0       0.429732      0.067163       0.300900   
median_age          102.0      41.600000      6.418306      21.500000   
tt_work             102.0    2336.284314   1253.010603     771.000000   

                              25%            50%            75%            max  
median_house_value  663400.000000  708200.000000  809225.000000  930600.000000  
pct_white                0.752426       0.853539       0.895305       0.966185  
pct_rented               0.143687       0.290194       0.471491       0.786959  
pct_hh_female            0.091919       0.103113       0.120754       0.169700  
pct_bachelor             0.000000       0.002871       0.005977       0.047335  
median_no_rooms          4.600000       6.000000       6.775000       7.900000  
income_gini              0.383750       0.424850       0.470500       0.666700  
median_age              37.200000      42.300000      46.400000      54.400000  
tt_work               1485.250000    2103.500000    2839.500000    8327.000000  

	---------
	Cluster 1
                    count           mean           std         min  \
median_house_value  240.0  500787.574698  57295.975694  414100.000   
pct_white           240.0       0.722390      0.149383       0.000   
pct_rented          240.0       0.405979      0.223881       0.000   
pct_hh_female       240.0       0.101796      0.030586       0.000   
pct_bachelor        240.0       0.010111      0.010658       0.000   
median_no_rooms     240.0       5.294402      0.957028       2.800   
income_gini         240.0       0.400898      0.048220       0.283   
median_age          240.0      37.210522      7.389658      16.500   
tt_work             240.0    2539.429167   1911.989514       0.000   

                              25%            50%            75%            max  
median_house_value  453750.000000  496850.000000  539675.000000  617000.000000  
pct_white                0.667674       0.754861       0.823160       0.946018  
pct_rented               0.225816       0.373094       0.541326       1.000000  
pct_hh_female            0.091103       0.107624       0.119297       0.166396  
pct_bachelor             0.002472       0.006677       0.015632       0.088600  
median_no_rooms          4.700000       5.400000       6.000000       7.500000  
income_gini              0.369650       0.403850       0.428775       0.553800  
median_age              33.650000      37.100000      41.050000      72.500000  
tt_work               1679.750000    2266.500000    3041.250000   24143.000000  

	---------
	Cluster 2
                    count          mean            std            min  \
median_house_value   25.0  1.168004e+06  120618.166404  972800.000000   
pct_white            25.0  8.410426e-01       0.109941       0.542849   
pct_rented           25.0  2.875354e-01       0.159089       0.089633   
pct_hh_female        25.0  1.068837e-01       0.017825       0.068381   
pct_bachelor         25.0  2.245429e-03       0.002630       0.000000   
median_no_rooms      25.0  6.020000e+00       1.333229       4.000000   
income_gini          25.0  4.741920e-01       0.042624       0.408200   
median_age           25.0  4.443600e+01       9.200674      24.800000   
tt_work              25.0  2.182000e+03    1321.458790     702.000000   

                             25%           50%           75%           max  
median_house_value  1.068400e+06  1.174600e+06  1.256300e+06  1.391700e+06  
pct_white           8.087572e-01  8.755782e-01  9.118435e-01  9.461056e-01  
pct_rented          1.525540e-01  2.407136e-01  4.432393e-01  6.360000e-01  
pct_hh_female       9.007014e-02  1.075795e-01  1.230421e-01  1.303158e-01  
pct_bachelor        0.000000e+00  1.354211e-03  4.228968e-03  8.325624e-03  
median_no_rooms     4.800000e+00  6.200000e+00  7.300000e+00  7.800000e+00  
income_gini         4.455000e-01  4.692000e-01  5.032000e-01  5.798000e-01  
median_age          4.000000e+01  4.210000e+01  5.220000e+01  5.950000e+01  
tt_work             1.207000e+03  1.943000e+03  2.900000e+03  6.686000e+03  

	---------
	Cluster 3
                    count          mean            std           min  \
median_house_value    9.0  1.876867e+06  177646.007321  1.583300e+06   
pct_white             9.0  9.088069e-01       0.039216  8.349267e-01   
pct_rented            9.0  2.514381e-01       0.098227  1.245819e-01   
pct_hh_female         9.0  1.169440e-01       0.023233  7.169202e-02   
pct_bachelor          9.0  2.002206e-03       0.003244  0.000000e+00   
median_no_rooms       9.0  6.422222e+00       0.909365  5.100000e+00   
income_gini           9.0  5.203444e-01       0.032293  4.737000e-01   
median_age            9.0  5.054444e+01       4.640612  4.310000e+01   
tt_work               9.0  1.237778e+03     368.902690  7.540000e+02   

                             25%           50%           75%           max  
median_house_value  1.675900e+06  2.000001e+06  2.000001e+06  2.000001e+06  
pct_white           8.880597e-01  9.072782e-01  9.360583e-01  9.596588e-01  
pct_rented          1.302682e-01  2.954939e-01  3.264249e-01  3.544750e-01  
pct_hh_female       1.033566e-01  1.262837e-01  1.280858e-01  1.456606e-01  
pct_bachelor        0.000000e+00  0.000000e+00  2.895753e-03  9.871245e-03  
median_no_rooms     5.800000e+00  6.600000e+00  7.100000e+00  7.900000e+00  
income_gini         5.048000e-01  5.110000e-01  5.280000e-01  5.763000e-01  
median_age          4.860000e+01  5.100000e+01  5.280000e+01  5.800000e+01  
tt_work             8.910000e+02  1.345000e+03  1.401000e+03  1.919000e+03  

	---------
	Cluster 4
                    count           mean           std           min  \
median_house_value  252.0  326728.968254  75348.415638  17900.000000   
pct_white           252.0       0.657904      0.165296      0.126219   
pct_rented          252.0       0.512041      0.214248      0.095565   
pct_hh_female       252.0       0.106273      0.023002      0.022989   
pct_bachelor        252.0       0.019860      0.013689      0.000000   
median_no_rooms     252.0       4.626587      0.813462      2.600000   
income_gini         252.0       0.402220      0.048714      0.287600   
median_age          252.0      34.246032      5.825171     24.400000   
tt_work             252.0    2187.753968    732.742273    592.000000   

                              25%            50%            75%            max  
median_house_value  306575.000000  346150.000000  373075.000000  413400.000000  
pct_white                0.576520       0.692083       0.774278       0.951479  
pct_rented               0.327662       0.520989       0.692279       0.946188  
pct_hh_female            0.092775       0.105000       0.120620       0.161811  
pct_bachelor             0.008788       0.018112       0.027612       0.069847  
median_no_rooms          4.000000       4.550000       5.300000       6.300000  
income_gini              0.369150       0.395500       0.428975       0.558100  
median_age              30.200000      33.550000      36.450000      64.800000  
tt_work               1689.000000    2109.500000    2682.000000    4915.000000  
</pre></div>
</div>
</div>
</div>
<p>However, this approach quickly gets out of hand: more detailed profiles can simply
return to an unwieldy mess of numbers. A better approach to constructing
cluster profiles is be to draw the distributions of cluster members’ data.
To do this we need to “tidy up” the dataset. A tidy dataset <span id="id4">[<a class="reference internal" href="references.html#id2"><span>W+14</span></a>]</span>
is one where every row is an observation, and every column is a variable. Thus,
a few steps are required  to tidy up our labeled data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Index db on cluster ID</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;k5cls&#39;</span><span class="p">)</span>
<span class="c1"># Keep only variables used for clustering</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">]</span>
<span class="c1"># Stack column names into a column, obtaining </span>
<span class="c1"># a &quot;long&quot; version of the dataset</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="c1"># Take indices into proper columns</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1"># Rename column names</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;level_1&#39;</span><span class="p">:</span> <span class="s1">&#39;Attribute&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;Values&#39;</span><span class="p">}</span>
<span class="p">)</span>
<span class="c1"># Check out result</span>
<span class="n">tidy_db</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k5cls</th>
      <th>Attribute</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>median_house_value</td>
      <td>732900.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>pct_white</td>
      <td>0.916988</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>pct_rented</td>
      <td>0.373913</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>pct_hh_female</td>
      <td>0.052896</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>pct_bachelor</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we are ready to plot. Below, we’ll show the distribution of each cluster’s values
for each variable. This gives us the full distributional profile of each cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup the facets</span>
<span class="n">facets</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">tidy_db</span><span class="p">,</span>
    <span class="n">col</span><span class="o">=</span><span class="s1">&#39;Attribute&#39;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;k5cls&#39;</span><span class="p">,</span>
    <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">aspect</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
<span class="c1"># Build the plot from `sns.kdeplot`</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">facets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">seaborn</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s1">&#39;Values&#39;</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_42_0.png" src="../_images/10_clustering_and_regionalization_42_0.png" />
</div>
</div>
<p>Note that we create the figure using the facetting functionality in <code class="docutils literal notranslate"><span class="pre">seaborn</span></code>, which
streamlines notably the process to create multi-plot figures whose dimensions and
content are data-driven. This happens in two steps: first, we set up the frame (<code class="docutils literal notranslate"><span class="pre">facets</span></code>),
and then we “map” a function (<code class="docutils literal notranslate"><span class="pre">seaborn.kdeplot</span></code>) to the data, within such frame.</p>
<p>The figure allows us to see that, while some attributes such as the percentage of
female households (<code class="docutils literal notranslate"><span class="pre">pct_hh_female</span></code>) display largely the same distribution for
each cluster, others paint a much more divided picture (e.g. <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code>).
Taken altogether, these graphs allow us to start delving into the multidimensional
complexity of each cluster and the types of areas behind them.</p>
</div>
</div>
<div class="section" id="hierarchical-clustering">
<h2>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h2>
<p>As mentioned above, k-means is only one clustering algorithm. There are
plenty more. In this section, we will take a similar look at the San Diego
dataset using another staple of the clustering toolkit: agglomerative
hierarchical clustering (AHC). Agglomerative clustering works by building a hierarchy of
clustering solutions that starts with all singletons (each observation is a single
cluster in itself) and ends with all observations assigned to the same cluster.
These extremes are not very useful in themselves. But, in between, the hierarchy
contains many distinct clustering solutions with varying levels of detail.
The intuition behind the algorithm is also rather straightforward:</p>
<ol class="simple">
<li><p>begin with everyone as part of its own cluster;</p></li>
<li><p>find the two closest observations based on a distance metric (e.g. euclidean);</p></li>
<li><p>join them into a new cluster;</p></li>
<li><p>repeat steps 2) and 3) until reaching the degree of aggregation desired.</p></li>
</ol>
<p>The algorithm is thus called “agglomerative”
because it starts with individual clusters and “agglomerates” them into fewer
and fewer clusters containing more and more observations each. Also, like with
k-means, AHC does require the user to specify a number of clusters in advance.
This is because, following from the mechanism the method has to build clusters,
AHC can provide a solution with as many clusters as observations (<span class="math notranslate nohighlight">\(k=n\)</span>),
or with a only one (<span class="math notranslate nohighlight">\(k=1\)</span>).</p>
<p>Enough of theory, let’s get coding! In Python, AHC can be run
with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> in very much the same way we did for k-means in the previous
section. First we need to import it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
</pre></div>
</div>
</div>
</div>
<p>In this case, we use the <code class="docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code> class and again
use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to actually apply the clustering algorithm to our data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Iniciate the algorithm</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># Run clustering</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
<span class="c1"># Assign labels to main data table</span>
<span class="n">db</span><span class="p">[</span><span class="s1">&#39;ward5&#39;</span><span class="p">]</span> <span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>
</div>
</div>
</div>
<p>As above, we can check the number of observations that fall within each cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ward5sizes</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;ward5&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">ward5sizes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ward5
0    141
1    233
2    222
3     23
4      9
dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Further, we can check the simple average profiles of our clusters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ward5means</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;ward5&#39;</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">ward5means</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>ward5</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>median_house_value</th>
      <td>703765.957</td>
      <td>473097.931</td>
      <td>316161.712</td>
      <td>1184173.913</td>
      <td>1876867.222</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.799</td>
      <td>0.706</td>
      <td>0.656</td>
      <td>0.842</td>
      <td>0.909</td>
    </tr>
    <tr>
      <th>pct_rented</th>
      <td>0.327</td>
      <td>0.420</td>
      <td>0.523</td>
      <td>0.293</td>
      <td>0.251</td>
    </tr>
    <tr>
      <th>pct_hh_female</th>
      <td>0.105</td>
      <td>0.102</td>
      <td>0.106</td>
      <td>0.107</td>
      <td>0.117</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.005</td>
      <td>0.011</td>
      <td>0.021</td>
      <td>0.002</td>
      <td>0.002</td>
    </tr>
    <tr>
      <th>median_no_rooms</th>
      <td>5.695</td>
      <td>5.222</td>
      <td>4.575</td>
      <td>5.939</td>
      <td>6.422</td>
    </tr>
    <tr>
      <th>income_gini</th>
      <td>0.421</td>
      <td>0.401</td>
      <td>0.403</td>
      <td>0.478</td>
      <td>0.520</td>
    </tr>
    <tr>
      <th>median_age</th>
      <td>41.535</td>
      <td>36.389</td>
      <td>34.062</td>
      <td>44.261</td>
      <td>50.544</td>
    </tr>
    <tr>
      <th>tt_work</th>
      <td>2305.206</td>
      <td>2556.399</td>
      <td>2172.563</td>
      <td>2201.913</td>
      <td>1237.778</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And again, we can tidy our dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Index db on cluster ID</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;ward5&#39;</span><span class="p">)</span>
<span class="c1"># Keep only variables used for clustering</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">]</span>
<span class="c1"># Stack column names into a column, obtaining </span>
<span class="c1"># a &quot;long&quot; version of the dataset</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="c1"># Take indices into proper columns</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1"># Rename column names</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;level_1&#39;</span><span class="p">:</span> <span class="s1">&#39;Attribute&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;Values&#39;</span><span class="p">}</span>
<span class="p">)</span>
<span class="c1"># Check out result</span>
<span class="n">tidy_db</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ward5</th>
      <th>Attribute</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>median_house_value</td>
      <td>732900.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>pct_white</td>
      <td>0.916988</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>pct_rented</td>
      <td>0.373913</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>pct_hh_female</td>
      <td>0.052896</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>pct_bachelor</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And create a plot of the profiles’ distributions:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup the facets</span>
<span class="n">facets</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">tidy_db</span><span class="p">,</span>
    <span class="n">col</span><span class="o">=</span><span class="s1">&#39;Attribute&#39;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;ward5&#39;</span><span class="p">,</span> 
    <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">aspect</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
<span class="c1"># Build the plot as a `sns.kdeplot`</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">facets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">seaborn</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s1">&#39;Values&#39;</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_54_0.png" src="../_images/10_clustering_and_regionalization_54_0.png" />
</div>
</div>
<p>For the sake of brevity, we will not spend much time on the plots above.
However, the interpretation is analogous to that of the k-means example.</p>
<p>On the spatial side, we can explore the geographical dimension of the
clustering solution by making a map of the clusters. To make the comparison
with k-means simpler, we will display both side by side:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">db</span><span class="p">[</span><span class="s1">&#39;ward5&#39;</span><span class="p">]</span> <span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c1"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

            <span class="c1">### K-Means ###</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">column</span><span class="o">=</span><span class="s1">&#39;ward5&#39;</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Set2&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
<span class="p">)</span>
<span class="c1"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c1"># Add title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;K-Means solution ($k=5$)&#39;</span><span class="p">)</span>

            <span class="c1">### AHC ###</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">column</span><span class="o">=</span><span class="s1">&#39;k5cls&#39;</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Set3&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
<span class="p">)</span>
<span class="c1"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c1"># Add title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;AHC solution ($k=5$)&#39;</span><span class="p">)</span>

<span class="c1"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_56_0.png" src="../_images/10_clustering_and_regionalization_56_0.png" />
</div>
</div>
<p>While we must remember our earlier caveat about how irregular polygons can
baffle our visual intuition, a closer visual inspection of the cluster geography
suggests a clear pattern: although they are not identical, both clustering solutions capture
very similar overall spatial structure. Furthermore, both solutions slightly violate
Tobler’s law in the sense all of the clusters have disconnected components. The five
multivariate clusters in each case are actually composed of many disparate
geographical areas, strewn around the map according only to the structure of the
data and not its geography. That is, in order to travel to
every tract belonging to a cluster, we would have to journey through
other clusters as well.</p>
</div>
<div class="section" id="spatially-constrained-hierarchical-clustering">
<h2>Spatially Constrained Hierarchical Clustering<a class="headerlink" href="#spatially-constrained-hierarchical-clustering" title="Permalink to this headline">¶</a></h2>
<div class="section" id="contiguity-constraint">
<h3>Contiguity constraint<a class="headerlink" href="#contiguity-constraint" title="Permalink to this headline">¶</a></h3>
<p>Fragmented clusters are not intrinsically invalid, particularly if we are
interested in exploring the overall structure and geography of multivariate
data. However, in some cases, the application we are interested in might
require that all the observations in a class be spatially connected. For
example, when detecting communities or neighborhoods (as is sometimes needed when
drawing electoral or census boundaries), they are nearly always distinct
self-connected areas, unlike our clusters shown above. To ensure that clusters are
not spatially fragmented, we turn to regionalization.</p>
<p>Regionalization methods are clustering techniques that impose a spatial constraint
on clusters. In other words, the result of a regionalization algorithm contains clusters with
areas that are geographically coherent, in addition to having coherent data profiles.
Effectively, this means that regionalization methods construct clusters that are
all internally-connected; these are the <em>regions</em>. Thus, a regions’ members must
be geographically <em>nested</em> within the region’s boundaries.</p>
<p>This type of nesting relationship is easy to identify
in the real world. Census geographies provide good examples: counties nest within states
in the US; or local super output areas (LSOAs) nest within middle super output areas
(MSOAs) in the UK.
The difference between these real-world nestings and the output of a regionalization
algorithm is that the real-world nestings are aggregated according to administrative
principles, while regions’ members are aggregated according to statistical similarity. In the same manner as the
clustering techniques explored above, these regionalization methods aggregate
observations that are similar in their attributes; the profiles of regions are useful
in a similar manner as the profiles of clusters. But, in regionalization, the
clustering is also spatially constrained, so the region profiles and members will
likely be different from the unconstrained solutions.</p>
<p>As in the non-spatial case, there are many different regionalization methods.
Each has a different way to measure (dis)similarity, how the similarity is used
to assign labels, how these labels are iteratively adjusted, and so on. However,
as with clustering algorithms, regionalization methods all share a few common traits.
In particular, they all take a set of input attributes and a representation of
spatial connectivity in the form of a binary spatial weights matrix. Depending
on the algorithm, they also require the desired number of output regions. For
illustration, we will take the AHC algorithm we have just used above, and apply
an additional spatial constraint. In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, this is done using
our spatial weights matrix as a <code class="docutils literal notranslate"><span class="pre">connectivity</span></code> option.
This parameter will force the agglomerative algorithm to only allow observations to be grouped
in a cluster if they are also spatially connected:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="c1"># Specify cluster model with spatial constraint</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span>
    <span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">,</span> <span class="n">connectivity</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
<span class="c1"># Fit algorithm to the data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AgglomerativeClustering(connectivity=&lt;628x628 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;
	with 4016 stored elements in Compressed Sparse Row format&gt;,
                        n_clusters=5)
</pre></div>
</div>
</div>
</div>
<p>Let’s inspect the output:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">db</span><span class="p">[</span><span class="s1">&#39;ward5wq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c1"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c1"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s1">&#39;ward5wq&#39;</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c1"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c1"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_60_0.png" src="../_images/10_clustering_and_regionalization_60_0.png" />
</div>
</div>
<p>Introducing the spatial constraint results in fully-connected clusters with much
more concentrated spatial distributions. From an initial visual impression, it might
appear that our spatial constraint has been violated: there are tracts for both cluster 0 and
cluster 1 that appear to be disconnected from the rest of their clusters.
However, closer inspection reveals that each of these tracts is indeed connected
to another tract in its own cluster by very narrow shared boundaries.</p>
</div>
<div class="section" id="changing-the-spatial-constraint">
<h3>Changing the spatial constraint<a class="headerlink" href="#changing-the-spatial-constraint" title="Permalink to this headline">¶</a></h3>
<p>The spatial constraint in regionalization algorithms is structured by the
spatial weights matrix we use. An interesting
question is thus how the choice of weights influences the final region structure.
Fortunately, we can directly explore the impact that a change in the spatial weights matrix has on
regionalization. To do so, we use the same attribute data
but replace the Queen contiguity matrix with a spatial k-nearest neighbor matrix,
where each observation is connected to its four nearest observations, instead
of those it touches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">KNN</span><span class="o">.</span><span class="n">from_dataframe</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With this matrix connecting each tract to the four closest tracts, we can run
another AHC regionalization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="c1"># Specify cluster model with spatial constraint</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span>
    <span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">,</span> <span class="n">connectivity</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
<span class="c1"># Fit algorithm to the data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AgglomerativeClustering(connectivity=&lt;628x628 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;
	with 2512 stored elements in Compressed Sparse Row format&gt;,
                        n_clusters=5)
</pre></div>
</div>
</div>
</div>
<p>And plot the final regions:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">db</span><span class="p">[</span><span class="s1">&#39;ward5wknn&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c1"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c1"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s1">&#39;ward5wknn&#39;</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c1"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c1"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10_clustering_and_regionalization_66_0.png" src="../_images/10_clustering_and_regionalization_66_0.png" />
</div>
</div>
<p>Even though we have specified a spatial constraint, the constraint applies to the
connectivity graph modeled by our weights matrix. Therefore, using k-nearest neighbors
to constrain the agglomerative clustering may not result in regions that are connected
according to a different connectivity rule, such as the queen contiguity rule used
in the previous section. However, the regionalization here is fortuitous; even though
we used the 4-nearest tracts to constrain connectivity, all but one of the clusters,
cluster 4, is <em>also</em> connected according to our earlier queen contiguity rule.</p>
<p>At first glance, this may seem counter-intuitive. We did specify the spatial
constraint, so our initial reaction is that the connectivity constraint is
violated. However, this is not the case, since the constraint applies to the
k-nearest neighbor graph, not the queen contiguity graph. Therefore, since tracts
in this solution are considered as connected to their four closest neighbors,
clusters can “leapfrog” over one another. Thus, it is important to recognize that
the apparent spatial structure of regionalizations will depend on how the
connectivity of observations is modeled.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>Overall, clustering and regionalization are two complementary tools to reduce
complexity in multivariate data and build better understandings of their spatial structure.
Often, there is simply too much data to examine every variables’ map and its
relation to all other variable maps.
Thus, clustering reduces this complexity into a single conceptual shorthand by which
people can easily describe complex and multifaceted data.
Clustering constructs groups of observations (called <em>clusters</em>)
with coherent <em>profiles</em>, or distinct and internally-consistent
distributional/descriptive characteristics.
These profiles are the conceptual shorthand, since members of each cluster should
be more similar to the cluster at large than they are to any other cluster.
Many different clustering methods exist; they differ on how the “cluster at large”
is defined, and how “similar” members must be to clusters, or how these clusters
are obtained.
Regionalization is a special kind of clustering that imposes an additional geographic requirement.
Observations should be grouped so that each spatial cluster,
or <em>region</em>, is spatially-coherent as well as data-coherent.
Thus, regionalization is often concerned with connectivity in a contiguity
graph for data collected in areas; this ensures that the regions that are identified
are fully internally-connected.
However, since many regionalization methods are defined for an arbitrary connectivity structure,
these graphs can be constructed according to different rules as well, such as the k-nearest neighbor graph.</p>
<p>In this chapter, we discussed the conceptual basis for clustering and regionalization,
as well as showing why clustering is done.
Further, we have demonstrated how to build clusters using a combination of (geographic) data
science packages, and how to interrogate the meaning of these clusters as well.
More generally, clusters are often used in predictive and explanatory settings,
in addition to being used for exploratory analysis in their own right.
Clustering and regionalization are intimately related to the analysis of spatial autocorrelation as well,
since the spatial structure and covariation in multivariate spatial data is what
determines the spatial structure and data profile of discovered clusters or regions.
Thus, clustering and regionalization are essential tools for the geographic data scientist.</p>
</div>
<div class="section" id="questions">
<h2>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>What disciplines employ regionalization? Cite concrete examples for each discipline you list.</p></li>
<li><p>Contrast and compare  the concepts of <em>clusters</em> and <em>regions</em>?</p></li>
<li><p>In evaluating the quality of the solution to a regionalization problem, how might traditional measures of cluster evaluation be used? In what ways might those measures be limited and need expansion to consider the geographical dimensions of the problem?</p></li>
<li><p>Discuss the implications for the processes of regionalization that follow from the number of <em>connected components</em> in the spatial weights matrix that would be used.</p></li>
<li><p>Consider two possible weights matrices for use in a spatially constrained clustering problem. Both form a single connected component for all the areal units. However, they differ in the sparsity of their adjacency graphs (think Rook being less dense than Queen graphs).
a. How might the sparsity of the weights matrix affect the quality of the clustering solution?
b. Using <code class="docutils literal notranslate"><span class="pre">pysal.lib.weights.higher_order</span></code>, construct a second-order adjacency matrix of the weights matrix used in this chapter.
c. Compare the <code class="docutils literal notranslate"><span class="pre">pct_nonzero</span></code> for both matrices.
d. Rerun the analysis from this chapter using this new second-order weights matrix. What changes?</p></li>
<li><p>The idea of spatial dependence, that near things tend to be more related than distant things, is an extensively-studied property of spatial data. How might solutions to clustering and regionalization problems change if dependence is very strong and positive? very weak? very strong and negative?</p></li>
</ol>
<hr class="docutils" />
<p><strong>DAB</strong>: I don’t understand this so I’d suggest to rephrase it or remove it</p>
<ol class="simple">
<li><p>In other areas of spatial analysis, multilevel models [1] recognize that sometimes, geographical regions are more similar internally than they are externally. That is, two observations in the same region are probably more similar than two observations in different regions. If this kind of dependence is very strong, what would happen to clustering and regionalization solutions?</p></li>
</ol>
<hr class="docutils" />
<ol class="simple">
<li><p>Using a spatial weights object obtained as <code class="docutils literal notranslate"><span class="pre">w</span> <span class="pre">=</span> <span class="pre">pysal.lib.weights.lat2W(20,20)</span></code>, what are the number of unique ways to partition the graph into 20 clusters of 20 units each, subject to each cluster being a connected component? What are the unique number of possibilities for <code class="docutils literal notranslate"><span class="pre">w</span> <span class="pre">=</span> <span class="pre">pysal.lib.weights.lat2W(20,20,</span> <span class="pre">rook=False)</span></code> ?</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="09_spatial_inequality.html" title="previous page">Spatial Inequality</a>
    <a class='right-next' id="next-link" href="11_regression.html" title="next page">Spatial Regression</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sergio J. Rey, Dani Arribas-Bel, Levi J. Wolf<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-146598819-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>